# Calculus {#derivatives}

Calculus is a fundamental part of any type of statistics exercise. Although you may not be taking derivatives and integral in your daily work as an analyst, calculus undergirds many concepts we use: maximization, expectation, etc

## Example: The Mean is a Type of Integral {-}


## Derivatives {#derivintro}

The derivative of $f$ at $x$ is its rate of change at $x$: how much $f(x)$ changes with a change in $x$. The rate of change is a fraction --- rise over run --- but because not all lines are straight and the rise over run formula will give us different values depending on the range we examine, we need to take a limit (Section \@ref(limits)).


```{definition, name = "Derivative"}
Let $f$ be a function whose domain includes an open interval containing the point $x$.  The derivative of $f$ at $x$ is given by

\[\frac{d}{dx}f(x) =\lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{(x+h)-x} = \lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{h}
\]

There are a two main ways to denote a derivate:
  
* Leibniz Notation: $\frac{d}{dx}(f(x)$
* Prime or Lagrange Notation: $f'(x)$


```

If $f(x)$ is a straight line, the derivative is the slope. For a curve, the slope changes by the values of $x$, so the derivative is the slope of the line tangent to the curve at $x$.

If $f'(x)$ exists at a point $x_0$, then $f$ is said to be __differentiable__ at $x_0$. That also implies that $f(x)$ is continuous at $x_0$. 



### Properties of derivatives  {-}
Suppose that $f$ and $g$ are differentiable at $x$ and that $\alpha$ is a constant.  Then the functions $f\pm g$, $\alpha f$, $f g$, and $f/g$ (provided $g(x)\ne 0$) are also differentiable at $x$.  Additionally,

__Constant rule:__ \[\left[k f(x)\right]' = k f'(x)\]

__Sum rule:__ \[\left[f(x)\pm g(x)\right]' = f'(x)\pm g'(x)\]

With a bit more algebra, we can apply the definition of derivatives to get a formula for of the derivative of a product and a derivative of a quotient.

__Product rule:__ \[\left[f(x)g(x)\right]' = f'(x)g(x)+f(x)g'(x)\]

__Quotient rule:__  \[\left[f(x)/g(x)\right]' = \frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}, g(x)\neq 0\]


Finally,  one way to think of the power of derivatives is that it takes a function a notch down in complexity. The power rule applies to any higher-order function:

__Power rule:__ \[\left[x^k\right]' = k x^{k-1}\]

The power rule is proved by induction.


These "rules"	become apparent by applying the definition of the derivative above to each of the things to be "derived", but these come up so frequently that it is best to repeat until it is muscle memory. 


```{exercise, name="Derivative of Polynomials"} 

For each of the following functions, find the first-order derivative $f^\prime(x)$. 


1. $f(x)=c$
1. $f(x)=x$
1. $f(x)=x^2$
1. $f(x)=x^3$
1. $f(x)=\frac{1}{x^2}$
1. $f(x)=(x^3)(2x^4)$
1. $f(x) = x^4 - x^3 + x^2 - x + 1$
1. $f(x) = (x^2 + 1)(x^3 - 1)$
1. $f(x) = 3x^2 + 2x^{1/3}$
1. $f(x)=\frac{x^2+1}{x^2-1}$
  

```


## Higher-Order Derivatives (Derivatives of Derivatives of Derivatives) {#derivpoly}

The first derivative is applying the definition of derivatives on the function, and it can be expressed as

\[f'(x),  ~~ y',  ~~ \frac{d}{dx}f(x), ~~ \frac{dy}{dx}\]

We can keep applying the differentiation process to functions that are themselves derivatives.  The derivative of $f'(x)$ with respect to $x$, would then be $$f''(x)=\lim\limits_{h\to 0}\frac{f'(x+h)-f'(x)}{h}$$ and we can therefore call it the __Second derivative:__ 


\[f''(x), ~~ y'', ~~ \frac{d^2}{dx^2}f(x), ~~ \frac{d^2y}{dx^2}\]


Similarly, the derivative of $f''(x)$ would be called the third derivative and is denoted $f'''(x)$. And by extension, the __nth derivative__  is expressed as  $\frac{d^n}{dx^n}f(x)$, $\frac{d^ny}{dx^n}$.


```{example, name="Succession of Derivatives"}
\begin{align*}
f(x) &=x^3\\
f^{\prime}(x) &=3x^2\\
f^{\prime\prime}(x) &=6x \\
f^{\prime\prime\prime}(x) &=6\\
f^{\prime\prime\prime\prime}(x) &=0\\
\end{align*}
```

Earlier, in Section \ref@(derivintro), we said that if a function differentiable at a given point, then it must be continuous. Further, if $f'(x)$ is itself continuous, then $f(x)$ is called continuously differentiable. All of this matters because many of our findings about optimization (Section \@ref(optim)) rely on differentiation, and so we want our function to be differentiable in as many layers.  A function that is continuously differentiable infinitly is called "smooth". Some examples: $f(x) = x^2$, $f(x) = e^x$. 


## Composite Functions and the Chain Rule

__Composite functions__ are formed by substituting one function into another and are denoted by \[(f\circ g)(x)=f[g(x)]\]  To form $f[g(x)]$, the range of $g$ must be contained (at least in part) within the domain of $f$. The domain of $f\circ g$ consists of all the points in the domain of $g$ for which $g(x)$ is in the domain of $f$.

```{example}
Let $f(x)=\log x$ for  $0<x<\infty$ and  $g(x)=x^2$ for $-\infty<x<\infty$.

Then 
\[(f\circ g)(x)=\log x^2, -\infty<x<\infty - \{0\}\]

Also 
\[(g\circ f)(x)=[\log x]^2, 0<x<\infty\]

Notice that $f\circ g$ and $g\circ f$ are not the same functions.
```



__Chain Rule__:  Let $y=(f\circ g)(x)= f[g(x)]$. The derivative of $y$ with respect to $x$ is \[\frac{d}{dx} \{ f[g(x)] \} = f'[g(x)] g'(x)\] which can also be written as \[\frac{dy}{dx}=\frac{dy}{dg(x)} \frac{dg(x)}{dx}\] (Note: the above does not imply that the $dg(x)$'s cancel out, as in fractions.  They are part of the derivative notation and you can't separate them out or cancel them.)

The chain rule can be thought of as the derivative of the "outside" times the derivative of the "inside", remembering that the derivative of the outside function is evaluated at the value of the inside function.
	
	

__Generalized Power Rule__:  If $f(x)=[g(x)]^k$, then $f^\prime(x) =k[g(x)]^{k-1}g^\prime(x)$.


```{example}
Find $f^\prime(x)$ for $f(x) = (3x^2+5x-7)^6$.

Approach: Let $f(z)=z^6$ and $z=g(x)=3x^2+5x-7$.  Then, $y=f[g(x)]$ and

\begin{align*}
\frac{dy}{dx}&=\\
\end{align*}
```



## Derivatives of Euler's number and natural logs

### Derivatives of natural exponential function ($e$) {-}


1. Derivative of $e^x$ is itself: $\frac{d}{dx}e^x = e^x$
2. Same thing if there were a constant in front: $\frac{d}{dx}\alpha e^x = \alpha e^x$
3. Same thing no matter how many derivatives there are in front: $\frac{d^n}{dx^n} \alpha e^x = \alpha e^x$
4. Chain Rule: When the exponent is a function of $x$, remember to take derivative of that function and add to product. $\frac{d}{dx}e^{u(x)}= e^{u(x)} u^\prime(x)$


```{example, name="Derivative of exponents"}
Find $dy/dx$ for the following.

1. $y=e^{-3x}$

    Let $u(x)=-3x$.  Then $u^\prime(x)=-3$ and $dy/dx=-3e^{-3x}$. 

2. $y=e^{x^2}$
  
    Let $u(x)=x^2$.  Then $u^\prime(x)=2x$ and $dy/dx=2xe^{x^2}$.

3. $y=e^{\sin 2x}$
  
    Let $u(x)=\sin 2x$.  Then $u^\prime(x) = 2\cos 2x$ and $dy/dx=(2\cos 2x) e^{\sin 2x}$
  
  
```


### Derivatives of $\log$ {-}

The natural log $\log$ (also written as $\ln$ to distinguish it from logarithms of diffferent bases) is the mirror image of the natural exponent and has mirroring properties.

1. log prime x is one over x: $\frac{d}{dx} \log x = \frac{1}{x}$
2. Exponents become multiplicative constants: $\frac{d}{dx} \log x^k = \frac{d}{dx} k \log x = \frac{k}{x}$
3. Chain rule again: $\frac{d}{dx} \log u(x) = \frac{u'(x)}{u(x)}\quad$
4. For any positive base $b$, $\frac{d}{dx} b^x = (\log b)\left(b^x\right)$.


Examples:  Find $dy/dx$ for

```{example, name="Derivative of logs"}
Find $dy/dx$ for the following.

1. $y=\log(x^2+9)$

    Let $u(x)=x^2+9$.  Then $u^\prime(x)=2x$ and \[\frac{dy}{dx}= \frac{u^\prime(x)}{u(x)} = \frac{2x}{(x^2+9)}\]
      
2. $y=\log(\log x)$

    Let $u(x)=\log x$.  Then $u^\prime(x)=1/x$ and $\frac{dy}{dx} = \frac{1}{(x\log x)}$.

3. $y=(\log x)^2$
  
    Use the generalized power rule. \[\frac{dy}{dx} = \frac{(2 \log x)}{x}\]

4. $y=\log e^x$ 

    We know that $\log e^x=x$ and that $dx/dx=1$, but we can double check.  Let $u(x)=e^x$.  Then $u^\prime(x)=e^x$ and $\frac{dy}{dx} = \frac{u^\prime(x)}{u(x)} = \frac{e^x}{e^x} = 1.$



```



## Partial Derivatives

Suppose we have a function $f$ now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find it's partial derivative, which is defined similar to the derivative of a function of one variable. 

__Partial Derivative__:  Let $f$ be a function of the variables $(x_1,\ldots,x_n)$.  The partial derivative of $f$ with respect to $x_i$ is 

\[\frac{\partial f}{\partial x_i} (x_1,\ldots,x_n) = \lim\limits_{h\to 0} \frac{f(x_1,\ldots,x_i+h,\ldots,x_n)-f(x_1,\ldots,x_i,\ldots,x_n)}{h}\]
Only the $i$th variable changes --- the others are treated as constants.

We can take higher-order partial derivatives, like we did with functions of a single variable, except now we the higher-order partials can be with respect to multiple variables.

```{example, name = "More than one type of partial"}
Notice that you can take partials with regard to different variables. 

Suppose $f(x,y)=x^2+y^2$. Then

\begin{align*}
\frac{\partial f}{\partial x}(x,y) &=\\
\frac{\partial f}{\partial y}(x,y) &=\\
\frac{\partial^2 f}{\partial x^2}(x,y) &=\\
\frac{\partial^2 f}{\partial x \partial y}(x,y) &=
\end{align*}
```


```{exercise}
Let $f(x,y)=x^3 y^4 +e^x -\log y$. What are the following partial derivaitves?

\begin{align*}
\frac{\partial f}{\partial x}(x,y) &=\\
\frac{\partial f}{\partial y}(x,y) &=\\
\frac{\partial^2 f}{\partial x^2}(x,y) &=\\
\frac{\partial^2 f}{\partial x \partial y}(x,y) &= 
\end{align*}
  

```



## L'Hopital's Rule

In studying limits, we saw that 
\[\lim\limits_{x \to c} \frac{f(x)}{g(x)} = \frac{\lim\limits_{x \to c} f(x)}{\lim\limits_{x \to c} g(x)},\]

provided that $\lim\limits_{x \to c} g(x)\ne 0$.
	
If both $\lim\limits_{x \to c} f(x)=0$ and $\lim\limits_{x \to c} g(x)=0$, then we get an __indeterminate form__ of the type $\frac{0}{0}$ as $x\to c$.  However, a limit may still exist. We can use L'Hopital's rule to find the limit.
	
__L'Hopital's Rule__:  Suppose $f$ and $g$ are differentiable on some interval $a<x<b$ and that either


1. $\lim\limits_{x\to a^+} f(x)=0$ and $\lim\limits_{x\to a^+} g(x)=0$, or
2. $\lim\limits_{x\to a^+} f(x)=\pm\infty$ and $\lim\limits_{x\to a^+} g(x)=\pm\infty$

Suppose further that $g'(x)$ is never zero on $a<x<b$ and that 
\[\lim\limits_{x\to a^+} \frac{f'(x)}{g'(x)}=L\] then
\[\lim\limits_{x\to a^+} \frac{f(x)}{g(x)}=L\]
	
Put more simply, if $\lim\limits_{x\to a f(x)}$ is of the form $0/0$ or $\pm \infty / \pm \infty$, then $$\lim\limits_{x\to a} f(x) = \lim\limits_{x\to a} f'(x)$$
	
And if $\lim\limits_{x\to a} \frac{f'(x)}{g'(x)} = 0/0$ or $\pm \infty / \pm \infty$ then you can apply L'H\^opital's rule a second time, and continue applying it until you have a solution.

```{example, name="LHopital"}
Use L'Hopital's rule to find the following limits:

1. $\lim\limits_{x\to 0^+}\frac{\log(1+x^2)}{x^3}$

2. $\lim\limits_{x\to  0^+} \frac{e^{1/x}}{1/x}$
		
3. $\lim\limits_{x\to 2} \frac{x-2}{(x+6)^{1/3}-2}$
  
  
```





## Taylor Series Approximation

__Taylor series__ (also known as the delta method) are used commonly to represent functions as infinite series of the function's derivatives at some point $a$. For example, Taylor series are very helpful in representing _nonlinear_ functions as linear functions. One can thus _approximate_ functions by using lower-order, finite series known as __Taylor polynomials__. If $a=0$, the series is called a Maclaurin series.
  

Specifically, a Taylor series of a real or complex function  $f(x)$ that is infinitely differentiable in the neighborhood of point $a$ is: 


\begin{align*}
	f(x) &= f(a) + \frac{f'(a)}{1!} (x-a) +  \frac{f''(a)}{2!} (x-a)^2 + \cdots\\
	 &= \sum_{n=0}^\infty \frac{f^{(n)} (a)}{n!} (x-a)^n
\end{align*}
  
__Taylor Approximation__: We can often approximate the curvature of a function $f(x)$ at point $a$ using a 2nd order Taylor polynomial around point $a$: 

\[f(x) = f(a) + \frac{f'(a)}{1!} (x-a) +  \frac{f''(a)}{2!} (x-a)^2
+ R_2\]


$R_2$ is the Lagrange remainder and often treated as negligible,
giving us:

\[f(x) \approx f(a) + f'(a)(x-a) +  \dfrac{f''(a)}{2} (x-a)^2\]

Taylor series expansion is easily generalized to multiple dimensions.

__Curvature and The Taylor Polynomial as a Quadratic Form__: 
The Hessian is used in a Taylor polynomial approximation to $f(\mathbf{x})$ and provides information about the curvature of $f({\bf x})$ at $\mathbf{x}$ --- e.g., which tells us whether a critical point $\mathbf{x}^*$ is a min, max, or saddle point.


1.  The second order Taylor polynomial about the critical point
${\bf x}^*$ is
  $$f({\bf x}^*+\bf h)=f({\bf x}^*)+\nabla f({\bf x}^*) \bf h +\frac{1}{2} \bf h^T
{\bf H(x^*)} \bf h + R(\bf h)$$
2.  Since we're looking at a critical point, $\nabla f({\bf x}^*)=0$;
and for small $\bf h$, $R(\bf h)$ is negligible.  Rearranging, we get
$$f({\bf x}^*+\bf h)-f({\bf x}^*)\approx \frac{1}{2} \bf h^T {\bf H(x^*)}
\bf h $$
3. The Righthand side here is a quadratic form and we can determine the definiteness of $\bf
H(x^*)$.


## Summary: Derivative calculus in 6 steps

With these six rules (and decent algebra and trigonometry skills) you can figure out the derivative of anything.

1. Sum rule: \[[f(x)\pm g(x)]' = f'(x)\pm g'(x)\]
2. Product rule: \[[f(x)g(x)]' = f'(x)g(x)+f(x)g'(x)\]
3. Power rule: \[[x^k]' = k x^{k-1}\]
4. Chain rule: \[\frac{d}{dx} \{ f[g(x)] \} = f'[g(x)] g'(x)\]
5. $e^x$: \[\frac{d}{dx} e^x = e^x\]
6. Trig identity: \[\frac{d}{dx} \sin(x) = \cos(x)\]



## The Indefinite Integral: The Antiderivative

So far, we've been interested in finding the derivative $g=f'$ of a function $f$.  However, sometimes we're interested in exactly the reverse:  finding the function $f$ for which $g$ is its derivative.  We refer to $f$ as the __antiderivative__ of $g$.

Let $DF$ be the derivative of $F$.  And let $DF(x)$ be the derivative of $F$ evaluated at $x$.  Then the antiderivative is denoted by $D^{-1}$ (i.e., the inverse derivative).  If $DF=f$, then $F=D^{-1}f$.

__Indefinite Integral__:  Equivalently, if $F$ is the antiderivative of $f$, then $F$ is also called the indefinite integral of $f$ and written $F(x)=\int\limits f(x)dx$.

```{example}
1. $\int\limits \frac{1}{x^2}dx=$
2. $\int\limits 3e^{3x}dx=$
3. $\int\limits (x^2-4) dx=$
  
```
\begin{comment}
\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{derinteg.eps}}}
\end{comment}

	
Notice from these examples that while there is only a single derivative for any function, there are multiple antiderivatives: one for any arbitrary constant $c$.  $c$ just shifts the curve up or down on the $y$-axis.  If more info is present about the antiderivative --- e.g., that it passes through a particular point --- then we can solve for a specific value of $c$.

## Common Rules of Integration


1. Constants are allowed to slip out: $\int a f(x)dx = a\int f(x)dx$
1. Integration of the sum is sum of integrations:  $\int [f(x)+g(x)]dx=\int f(x)dx + \int g(x)dx$
1. Reverse Power-rule:  $\int x^n dx = \frac{1}{n+1} x^{n+1} + c$
1. Exponents are still exponents:  $\int e^x dx = e^x +c$
1. Recall the derivative of $\log(x)$ is one over $x$, and so:   $\int \frac{1}{x} dx = \log x + c$
1. Reverse chain-rule:  $\int e^{f(x)}f^\prime(x)dx = e^{f(x)}+c$
1. More generally:  $\int [f(x)]^n f'(x)dx = \frac{1}{n+1}[f(x)]^{n+1}+c$
1. Remember the derivative of a log of a function:  $\int \frac{f^\prime(x)}{f(x)}dx=\log f(x) + c$


```{example, name="Common Integration"}
Simplify the following indefinite integrals:
  
* $\int 3x^2 dx =$
* $\int (2x+1)dx=$
* $\int e^x e^{e^x} dx =$
  
  
```





## The Definite Integral: The Area under the Curve

__Riemann Sum__:  Suppose we want to determine the area $A(R)$ of a region $R$ defined by a curve $f(x)$ and some interval $a\le x \le b$.  One way to calculate the area would be to divide the interval $a\le x\le b$ into $n$ subintervals of length $\Delta x$ and then approximate the region with a series of rectangles, where the base of each rectangle is $\Delta x$ and the height is $f(x)$ at the midpoint of that interval.  $A(R)$ would then be approximated by the area of the union of the rectangles, which is given by $$S(f,\Delta x)=\sum\limits_{i=1}^n f(x_i)\Delta x$$ and is called a Riemann sum.

As we decrease the size of the subintervals $\Delta x$, making the rectangles "thinner," we would expect our approximation of the area of the region to become closer to the true area.  This gives the limiting process $$A(R)=\lim\limits_{\Delta x\to 0}\sum\limits_{i=1}^n f(x_i)\Delta x$$

__Riemann Integral__:  If for a given function $f$ the Riemann sum approaches a limit as $\Delta x \to 0$, then that limit is called the Riemann integral of $f$ from $a$ to $b$.  Formally, $$\int\limits_a^b f(x) dx= \lim\limits_{\Delta x\to 0} \sum\limits_{i=1}^n f(x_i)\Delta x$$

__Definite Integral__: We use the notation $\int\limits_a^b f(x) dx$ to denote the definite integral of $f$ from $a$ to $b$.  In words, the definite integral $\int\limits_a^b f(x)dx$ is the area under the ``curve" f(x) from $x=a$ to $x=b$.

__First Fundamental Theorem of Calculus__:  Let the function $f$ be bounded on $[a,b]$ and continuous on $(a,b)$.  Then the function $$F(x)=\int\limits_a^x f(t)dt, \quad a\le x\le b$$ has a derivative at each point in $(a,b)$ and $$F'(x)=f(x), \quad a<x<b$$  This last point shows that differentiation is the inverse of integration.

__Second Fundamental Theorem of Calculus__:  Let the function $f$ be bounded on $[a,b]$ and continuous on $(a,b)$.  Let $F$ be any function that is continuous on $[a,b]$ such that $F'(x)=f(x)$ on $(a,b)$.  Then $$\int\limits_a^bf(x)dx = F(b)-F(a)$$

The procedure to calculate a simple definite integral $\int\limits_a^b f(x)dx$ is then

1. Find the indefinite integral $F(x)$.
2. Evaluate $F(b)-F(a)$.

```{example, name="Definite Integral of a monomial"}
Solve $\int\limits_1^3 3x^2 dx.$

Let $f(x) = 3x^2$. 

What is $F(x)$? From the power rule, recognize $\frac{d}{dx}x^3 = 3x^2$ so 
  
\begin{align*}
F(x) &= x^3\\
\int\limits_1^3 f(x) dx &= F(x = 3) - F(x  - 1)\\
&= 3^3 - 1^3\\
&=26
\end{align*}
```


```{exercise}
What is the value of $\int\limits_{-2}^2 e^x e^{e^x} dx$?
```



Properties of Definite Integrals:


1. There is no area below a point: \[\int\limits_a^a f(x)dx=0\]
2. Reversing the limits changes the sign of the integral: \[\int\limits_a^b f(x)dx=-\int\limits_b^a f(x)dx\]
3. Sums can be separated into their own integrals: \[\int\limits_a^b [\alpha f(x)+\beta g(x)]dx = \alpha \int\limits_a^b f(x)dx + \beta \int\limits_a^b g(x)dx\]
4. Areas can be combined as long as limits are linked: \[\int\limits_a^b f(x) dx +\int\limits_b^c f(x)dx = \int\limits_a^c f(x)dx\]


```{exercise, name="Definite integral shortcuts"}
Simplify the following definite intergrals.

1. $\int\limits_1^1 3x^2 dx =$ 
2. $\int\limits_0^4 (2x+1)dx=$ 
3. $\int\limits_{-2}^0 e^x e^{e^x} dx + \int\limits_0^2 e^x e^{e^x} dx =$
  
  
```



## Integration by Substitution

Sometimes the integrand doesn't appear integrable using common rules and antiderivatives.  A method one might try is __integration by substitution__, which is related to the Chain Rule.

Suppose we want to find the indefinite integral \[\int g(x)dx\] but $g(x)$ is complex and none of the formulas we have seen so far seem to apply immediately. The trick is to come up with a _new_ function $u(x)$ such that \[g(x)=f[u(x)]u'(x).\] 

Why does an introduction of yet another function end of simplifying things? Let's refer to the antiderivative of $f$ as $F$.  Then the chain rule tells us that \[\frac{d}{dx} F[u(x)]=f[u(x)]u'(x)\].  So, $F[u(x)]$ is the antiderivative of $g$.  We can then write $$\int g(x) dx= \int f[u(x)]u'(x)dx = \int \frac{d}{dx} F[u(x)]dx = F[u(x)]+c$$



To summarize, the procedure to determine the indefinite integral $\int g(x)dx$ by the method of substitution:

1. Identify some part of $g(x)$ that might be simplified by substituting in a single variable $u$ (which will then be a function of $x$).
2. Determine if $g(x)dx$ can be reformulated in terms of $u$ and $du$.
3. Solve the indefinite integral.
4. Substitute back in for $x$


Substitution can also be used to calculate a definite integral. Using the same procedure as above, \[\int\limits_a^b g(x)dx=\int\limits_c^d f(u)du = F(d)-F(c)\]
where $c=u(a)$ and $d=u(b)$.



```{example, name ="Integration by Substitution I"}

Solve $\int x^2 \sqrt{x+1}dx$.

\vspace{6pt}

The problem here is the $\sqrt{x+1}$ term.  However, if the integrand had $\sqrt{x}$ times some polynomial, then we'd be in business.  Let's try $u=x+1$.  Then $x=u-1$ and $dx=du$. Substituting these into the above equation, we get 

\begin{align*}
			\int x^2\sqrt{x+1}dx&= \int (u-1)^2\sqrt{u}du\\
			&= \int (u^2-2u+1)u^{1/2}du\\
			&= \int (u^{5/2}-2u^{3/2}+u^{1/2})du
\end{align*}

We can easily integrate this, since it is just a polynomial.  Doing so and substituting $u=x+1$ back in, we get \[\int x^2\sqrt{x+1}dx=2(x+1)^{3/2}\left[\frac{1}{7}(x+1)^2 -
\frac{2}{5}(x+1)+\frac{1}{3}\right]+c\]

```

For the above problem, we could have also used the substitution $u=\sqrt{x+1}$.  Then $x=u^2-1$ and $dx=2u du$.  Substituting these in, we get $$\int x^2\sqrt{x+1}dx=\int (u^2-1)^2 u 2u du$$ which when expanded is again a polynomial and gives the same result as above.


Another case in which integration by substitution is is useful is with a fraction.

```{example,name="Integration by Substitutiton II"}
Simplify \[\int\limits_0^1 \frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx\]

When an expression is raised to a power, it is often helpful to use this expression as the basis for a substitution.  So, let $u=1+e^{2x}$. Then $du=2e^{2x}dx$ and we can set $5e^{2x}dx=5du/2$.    Additionally, $u=2$ when $x=0$ and $u=1+e^2$ when $x=1$.  Substituting all of this in, we get

\begin{align*}
\int\limits_0^1 \frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx
			&= \frac{5}{2}\int\limits_2^{1+e^2}\frac{du}{u^{1/3}}\\
			&= \frac{5}{2}\int\limits_2^{1+e^2} u^{-1/3}du\\
			&= \left. \frac{15}{4} u^{2/3} \right|_2^{1+e^2}\\
			&= 9.53
\end{align*}

```



## Integration by Parts, or Ultraviolet Voodoo


Another useful integration technique is __integration by parts__, which is related to the Product Rule of differentiation. The product rule states that $$\frac{d}{dx}(uv)=u\frac{dv}{dx}+v\frac{du}{dx}$$  Integrating this and rearranging, we get $$\int u\frac{dv}{dx}dx= u v - \int v \frac{du}{dx}dx$$ or $$\int u(x) v'(x)dx=u(x)v(x) - \int v(x)u'(x)dx$$
More frequently remembered as $$\int u dv = u v - \int v du$$ where $du=u'(x)dx$ and $dv=v'(x)dx$.


For definite integrals: $\int\limits_a^b u\frac{dv}{dx}dx = \left. u v \right|_a^b - \int\limits_a^b v \frac{du}{dx}dx$

Our goal here is to find expressions for $u$ and $dv$ that, when substituted into the above equation, yield an expression that's more easily evaluated.


	
```{example,name="Integration by Parts"}
Simplify the following integrals. These seemingly obscure forms of integrals come up often when integrating distributions.

1. $\int x e^{ax} dx$
2. $\int x^n e^{ax} dx$
3. $\int x^3 e^{-x^2} dx$
  
  
```
