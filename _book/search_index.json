[
["index.html", "Math Prefresher for Political Scientists Chapter 1 About this Booklet", " Math Prefresher for Political Scientists Shiro Kuriwaki and Yon Soo Park 2018-06-19 Chapter 1 About this Booklet The documents in this booklet are the product of generations of Math (P)refresher Instructors: Curt Signorino 1996-1997; Ken Scheve 1997-1998; Eric Dickson 1998-2000; Orit Kedar 1999; James Fowler 2000-2001; Kosuke Imai 2001-2002; Jacob Kline 2002; Dan Epstein 2003; Ben Ansell 2003-2004; Ryan Moore 2004-2005; Mike Kellermann 2005-2006; Ellie Powell 2006-2007; Jen Katkin 2007-2008; Patrick Lam 2008-2009; Viridiana Rios 2009-2010; Jennifer Pan 2010-2011; Konstantin Kashin 2011-2012; Sol'{e} Prillaman 2013; Stephen Pettigrew 2013-2014; Anton Strezhnev 2014-2015; Mayya Komisarchik 2015-2016; Connor Jerzak 2016-2017; Shiro Kuriwaki 2017-2018; Yon Soo Park 2018- "],
["linear-algebra.html", "Chapter 2 Linear Algebra 2.1 Working with Vectors 2.2 Linear Independence 2.3 Basics of Matrix Algebra 2.4 Square Matrices 2.5 Linear Equations 2.6 Systems of Linear Equations 2.7 Systems of Equations as Matrices 2.8 Finding Solutions to Augmented Matrices and Systems of Equations 2.9 Rank — and Whether a System Has One, Infinite, or No Solutions 2.10 The Inverse of a Matrix 2.11 Linear Systems and Inverses 2.12 Determinants 2.13 Getting Inverse of a Matrix using its Determinant and Matrix of Cofactors 2.14 Inverse of Larger Matrices", " Chapter 2 Linear Algebra Topics: \\(\\bullet\\) Working with Vectors \\(\\bullet\\) Linear Independence \\(\\bullet\\) Basics of Matrix Algebra \\(\\bullet\\) Square Matrices \\(\\bullet\\) Linear Equations \\(\\bullet\\) Systems of Linear Equations \\(\\bullet\\) Systems of Equations as Matrices \\(\\bullet\\) Solving Augmented Matrices and Systems of Equations \\(\\bullet\\) Rank \\(\\bullet\\) The Inverse of a Matrix \\(\\bullet\\) Inverse of Larger Matrices Much of the material and examples for this lecture are taken from Gill (2006) , Simon &amp; Blume (1994) and Kolman (1993) . 2.1 Working with Vectors Vector: A vector in \\(n\\)-space is an ordered list of \\(n\\) numbers. These numbers can be represented as either a row vector or a column vector: \\[ {\\bf v} \\begin{pmatrix} v_1 &amp; v_2 &amp; \\dots &amp; v_n\\end{pmatrix} , {\\bf v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix}\\] We can also think of a vector as defining a point in \\(n\\)-dimensional space, usually \\({\\bf R}^n\\); each element of the vector defines the coordinate of the point in a particular direction. Vector Addition and Subtraction: If two vectors, \\({\\bf u}\\) and \\({\\bf v}\\), have the same length (i.e. have the same number of elements), they can be added (subtracted) together: \\[ {\\bf u} + {\\bf v} = \\begin{pmatrix} u_1 + v_1 &amp; u_2 + v_2 &amp; \\cdots &amp; u_k + v_n \\end{pmatrix}\\] \\[ {\\bf u} - {\\bf v} = \\begin{pmatrix} u_1 - v_1 &amp; u_2 - v_2 &amp; \\cdots &amp; u_k - v_n \\end{pmatrix}\\] Scalar Multiplication: The product of a scalar \\(c\\) (i.e. a constant) and vector \\({\\bf v}\\) is: \\[ c{\\bf v} = \\begin{pmatrix} cv_1 &amp; cv_2 &amp; \\dots &amp; cv_n \\end{pmatrix} \\] Vector Inner Product: The inner product (also called the dot product or scalar product) of two vectors \\({\\bf u}\\) and \\({\\bf v}\\) is again defined iff they have the same number of elements \\[ {\\bf u} \\cdot {\\bf v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n = \\sum_{i = 1}^n u_iv_i\\] If \\({\\bf u} \\cdot {\\bf v} = 0\\), the two vectors are orthogonal (or perpendicular). Vector Norm: The norm of a vector is a measure of its length. There are many different ways to calculate the norm, but the most common of is the Euclidean norm (which corresponds to our usual conception of distance in three-dimensional space): \\[ ||{\\bf v}|| = \\sqrt{{\\bf v}\\cdot{\\bf v}} = \\sqrt{ v_1v_1 + v_2v_2 + \\cdots + v_nv_n}\\] 2.2 Linear Independence Linear combinations: The vector \\({\\bf u}\\) is a linear combination of the vectors \\({\\bf v}_1, {\\bf v}_2, \\cdots , {\\bf v}_k\\) if \\[{\\bf u} = c_1{\\bf v}_1 + c_2{\\bf v}_2 + \\cdots + c_k{\\bf v}_k\\] Linear independence: A set of vectors \\({\\bf v}_1, {\\bf v}_2, \\cdots , {\\bf v}_k\\) is linearly independent if the only solution to the equation \\[c_1{\\bf v}_1 + c_2{\\bf v}_2 + \\cdots + c_k{\\bf v}_k = 0\\] is \\(c_1 = c_2 = \\cdots = c_k = 0\\). If another solution exists, the set of vectors is linearly dependent. A set \\(S\\) of vectors is linearly dependent iff at least one of the vectors in \\(S\\) can be written as a linear combination of the other vectors in \\(S\\). Linear independence is only defined for sets of vectors with the same number of elements; any linearly independent set of vectors in \\(n\\)-space contains at most \\(n\\) vectors. Exercises: Are the following sets of vectors linearly independent? 2.3 Basics of Matrix Algebra Matrix: A matrix is an array of real numbers arranged in \\(m\\) rows by \\(n\\) columns. The dimensionality of the matrix is defined as the number of rows by the number of columns, \\(m x n\\). \\[{\\bf A}=\\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix}\\] Note that you can think of vectors as special cases of matrices; a column vector of length \\(k\\) is a \\(k \\times 1\\) matrix, while a row vector of the same length is a \\(1 \\times k\\) matrix. It’s also useful to think of matrices as being made up of a collection of row or column vectors. For example, \\[\\bf A = \\begin{pmatrix} {\\bf a}_1 &amp; {\\bf a}_2 &amp; \\cdots &amp; {\\bf a}_m \\end{pmatrix}\\] Matrix Addition: Let \\(\\bf A\\) and \\(\\bf B\\) be two \\(m\\times n\\) matrices. \\[{\\bf A+B}=\\begin{pmatrix} a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; \\cdots &amp; a_{1n}+b_{1n} \\\\ a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; \\cdots &amp; a_{2n}+b_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1}+b_{m1} &amp; a_{m2}+b_{m2} &amp; \\cdots &amp; a_{mn}+b_{mn} \\end{pmatrix}\\] Note that matrices \\({\\bf A}\\) and \\({\\bf B}\\) must have the same dimensionality, in which case they are conformable for addition. Example: \\[{\\bf A}=\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix}, \\qquad {\\bf B}=\\begin{pmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 1 &amp; 2 \\end{pmatrix}\\] \\[{\\bf A+B}= \\phantom{\\begin{pmatrix} 2 &amp; 4 &amp; 4 \\\\ 6 &amp; 6 &amp; 8 \\end{pmatrix}}\\] Scalar Multiplication: Given the scalar \\(s\\), the scalar multiplication of \\(s {\\bf A}\\) is \\[ s {\\bf A}= s \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{pmatrix} = \\begin{pmatrix} s a_{11} &amp; s a_{12} &amp; \\cdots &amp; s a_{1n} \\\\ s a_{21} &amp; s a_{22} &amp; \\cdots &amp; s a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s a_{m1} &amp; s a_{m2} &amp; \\cdots &amp; s a_{mn} \\end{pmatrix}\\] Example: \\[s=2, \\qquad {\\bf A}=\\begin{pmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{pmatrix}\\] \\[s {\\bf A} = \\phantom{\\begin{pmatrix} 2 &amp; 4 &amp; 6 \\\\ 8 &amp; 10 &amp; 12 \\end{pmatrix}}\\] Matrix Multiplication: If \\({\\bf A}\\) is an \\(m\\times k\\) matrix and \\(\\bf B\\) is a \\(k\\times n\\) matrix, then their product \\(\\bf C = A B\\) is the \\(m\\times n\\) matrix where \\[c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\\cdots+a_{ik}b_{kj}\\] Examples: Note that the number of columns of the first matrix must equal the number of rows of the second matrix, in which case they are conformable for multiplication. The sizes of the matrices (including the resulting product) must be \\[(m\\times k)(k\\times n)=(m\\times n)\\] Also note that if AB exists, BA exists only if \\(\\dim({\\bf A}) = m \\times n\\) and \\(\\dim({\\bf B}) = n \\times m\\). This does not mean that AB = BA. AB = BA is true only in special circumstances, like when \\({\\bf A}\\) or \\({\\bf B}\\) is an identity matrix, \\({\\bf A} = {\\bf B}^{-1}\\), or \\({\\bf A} = {\\bf B}\\) and A is idempotent. Laws of Matrix Algebra: Commutative law for multiplication does not hold – the order of multiplication matters: \\[\\bf AB\\ne BA\\] Example: \\[{\\bf A}=\\begin{pmatrix} 1&amp;2\\\\-1&amp;3\\end{pmatrix}, \\qquad {\\bf B}=\\begin{pmatrix} 2&amp;1\\\\0&amp;1\\end{pmatrix}\\] \\[{\\bf AB}=\\begin{pmatrix} 2&amp;3\\\\-2&amp;2\\end{pmatrix}, \\qquad {\\bf BA}=\\begin{pmatrix} 1&amp;7\\\\-1&amp;3\\end{pmatrix}\\] Transpose: The transpose of the \\(m\\times n\\) matrix \\(\\bf A\\) is the \\(n\\times m\\) matrix \\({\\bf A}^T\\) (also written \\({\\bf A}&#39;\\)) obtained by interchanging the rows and columns of \\(\\bf A\\). Examples: The following rules apply for transposed matrices: Example of \\(({\\bf AB})^T = {\\bf B}^T{\\bf A}^T\\): \\[{\\bf A}=\\begin{pmatrix} 1&amp;3&amp;2\\\\2&amp;-1&amp;3\\end{pmatrix}, \\qquad {\\bf B}=\\begin{pmatrix} 0&amp;1\\\\2&amp;2\\\\3&amp;-1\\end{pmatrix}\\] \\[ ({\\bf AB})^T = \\left[ \\begin{pmatrix} 1&amp;3&amp;2\\\\2&amp;-1&amp;3\\end{pmatrix} \\begin{pmatrix} 0&amp;1\\\\2&amp;2\\\\3&amp;-1\\end{pmatrix} \\right]^T = \\begin{pmatrix} 12&amp;7\\\\5&amp;-3 \\end{pmatrix}\\] \\[ {\\bf B}^T{\\bf A}^T= \\begin{pmatrix} 0&amp;2&amp;3\\\\1&amp;2&amp;-1 \\end{pmatrix} \\begin{pmatrix} 1&amp;2\\\\3&amp;-1\\\\2&amp;3 \\end{pmatrix} = \\begin{pmatrix} 12&amp;7\\\\5&amp;-3 \\end{pmatrix}\\] 2.4 Square Matrices Square matrices have the same number of rows and columns; a \\(k \\times k\\) square matrix is referred to as a matrix of order \\(k\\). The diagonal of a square matrix is the vector of matrix elements that have the same subscripts. If A is a square matrix of order \\(k\\), then its diagonal is \\([ a_{11}, a_{22}, \\dots, a_{kk}]&#39;\\). There are several important types of square matrices: Identity Matrix: The \\(n\\times n\\) identity matrix \\({\\bf I}_n\\) is the matrix whose diagonal elements are 1 and all off-diagonal elements are 0. Examples: \\[ {\\bf I}_2=\\begin{pmatrix} 1&amp;0\\\\0&amp;1 \\end{pmatrix}, \\qquad {\\bf I}_3=\\begin{pmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1 \\end{pmatrix}\\] Symmetric Matrix: A matrix A is symmetric if \\({\\bf A} = {\\bf A}&#39;\\); this implies that \\(a_{ij} = a_{ji}\\) for all \\(i\\) and \\(j\\). Examples: \\[ {\\bf A} = \\begin{pmatrix} 1&amp;2\\\\2&amp;1 \\end{pmatrix} = {\\bf A}&#39;, \\qquad {\\bf B} =\\begin{pmatrix} 4&amp;2&amp;-1\\\\ 2&amp;1&amp;3\\\\ -1&amp;3&amp;1 \\end{pmatrix} = {\\bf B}&#39;\\] Diagonal Matrix: A matrix A is diagonal if all of its non-diagonal entries are zero; formally, if \\(a_{ij} = 0\\) for all \\(i \\neq j\\) Examples: \\[ {\\bf A} = \\begin{pmatrix} 1&amp;0\\\\0&amp;2 \\end{pmatrix}, \\qquad {\\bf B} =\\begin{pmatrix} 4&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1 \\end{pmatrix}\\] Triangular Matrix: A matrix is triangular one of two cases. If all entries below the diagonal are zero (\\(a_{ij} = 0\\) for all \\(i &gt; j\\)), it is upper triangular. Conversely, if all entries above the diagonal are zero (\\(a_{ij} = 0\\) for all \\(i &lt; j\\)), it is lower triangular. Examples: \\[ {\\bf A}_{LT}= \\begin{pmatrix} 1&amp;0 &amp; 0\\\\4&amp;2&amp;0\\\\-3 &amp; 2 &amp; 5 \\end{pmatrix}, \\qquad {\\bf A}_{UT}=\\begin{pmatrix} 1&amp;7&amp;-4\\\\ 0&amp;3&amp;9\\\\ 0&amp;0&amp;-3 \\end{pmatrix}\\] 2.5 Linear Equations Linear Equation: \\(a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\\) \\(a_i\\) are parameters or coefficients. \\(x_i\\) are variables or unknowns. Linear because only one variable per term and degree is at most 1. 2.6 Systems of Linear Equations We are often interested in solving linear systems like\\ \\[\\begin{matrix} x &amp; - &amp; 3y &amp; = &amp; -3\\\\ 2x &amp; + &amp; y &amp; = &amp; 8 \\end{matrix}\\] More generally, we might have a system of \\(m\\) equations in \\(n\\) unknowns \\[\\begin{matrix} a_{11}x_1 &amp; + &amp; a_{12}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{1n}x_n &amp; = &amp; b_1\\\\ a_{21}x_1 &amp; + &amp; a_{22}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{2n}x_n &amp; = &amp; b_2\\\\ \\vdots &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1}x_1 &amp; + &amp; a_{m2}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{mn}x_n &amp; = &amp; b_m \\end{matrix}\\] A solution to a linear system of \\(m\\) equations in \\(n\\) unknowns is a set of \\(n\\) numbers \\(x_1, x_2, \\cdots, x_n\\) that satisfy each of the \\(m\\) equations. Example: \\(x=3\\) and \\(y=2\\) is the solution to the above \\(2\\times 2\\) linear system. Notice from the graph that the two lines intersect at \\((3,2)\\). Does a linear system have one, no, or multiple solutions? For a system of 2 equations in 2 unknowns (i.e., two lines): Methods to solve linear systems: Substitution Elimination of variables Matrix methods 2.7 Systems of Equations as Matrices Matrices provide an easy and efficient way to represent linear systems such as \\[\\begin{matrix} a_{11}x_1 &amp; + &amp; a_{12}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{1n}x_n &amp; = &amp; b_1\\\\ a_{21}x_1 &amp; + &amp; a_{22}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{2n}x_n &amp; = &amp; b_2\\\\ \\vdots &amp; &amp; &amp; &amp; \\vdots &amp; &amp; &amp; \\vdots &amp; \\\\ a_{m1}x_1 &amp; + &amp; a_{m2}x_2 &amp; + &amp; \\cdots &amp; + &amp; a_{mn}x_n &amp; = &amp; b_m \\end{matrix}\\] as \\[{\\bf A x = b}\\] where Augmented Matrix: When we append \\(\\bf b\\) to the coefficient matrix \\(\\bf A\\), we get the augmented matrix \\(\\widehat{\\bf A}=[\\bf A | b]\\) \\[\\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\ \\vdots &amp; &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots\\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{pmatrix}\\] 2.8 Finding Solutions to Augmented Matrices and Systems of Equations Row Echelon Form: Our goal is to translate our augmented matrix or system of equations into row echelon form. This will provide us with the values of the vector x which solve the system. We use the row operations to change coefficients in lower triangle of the augmented matrix to 0. An augmented matrix of the form \\[\\begin{pmatrix} \\fbox{$a&#39;_{11}$}&amp; a&#39;_{12} &amp; a&#39;_{13}&amp; \\cdots &amp; a&#39;_{1n} &amp; | &amp; b&#39;_1\\\\ 0 &amp; \\fbox{$a&#39;_{22}$} &amp; a&#39;_{23}&amp; \\cdots &amp; a&#39;_{2n} &amp; | &amp; b&#39;_2\\\\ 0 &amp; 0 &amp; \\fbox{$a&#39;_{33}$}&amp; \\cdots &amp; a&#39;_{3n} &amp; | &amp; b&#39;_3\\\\ 0 &amp; 0 &amp;0 &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots \\\\ 0 &amp; 0 &amp;0 &amp;0 &amp; \\fbox{$a&#39;_{mn}$} &amp; | &amp; b&#39;_m \\end{pmatrix}\\] is said to be in row echelon form — each row has more leading zeros than the row preceding it. Reduced Row Echelon Form: We can go one step further and put the matrix into reduced row echelon form. Reduced row echelon form makes the value of x which solves the system very obvious. For a system of \\(m\\) equations in \\(m\\) unknowns, with no all-zero rows, the reduced row echelon form would be \\[\\begin{pmatrix} \\fbox{$1$} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; | &amp; b^*_1\\\\ 0 &amp; \\fbox{$1$} &amp; 0 &amp; 0 &amp; 0 &amp; | &amp; b^*_2\\\\ 0 &amp; 0 &amp; \\fbox{$1$} &amp; 0 &amp; 0 &amp; | &amp; b^*_3\\\\ 0 &amp; 0 &amp; 0 &amp;\\ddots &amp; 0 &amp; | &amp;\\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\fbox{$1$} &amp; | &amp; b^*_m \\end{pmatrix}\\] Gaussian and Gauss-Jordan elimination: We can conduct elementary row operations to get our augmented matrix into row echelon or reduced row echelon form. The methods of transforming a matrix or system into row echelon and reduced row echelon form are referred to as Gaussian elimination and Gauss-Jordan elimination, respectively. Elementary Row Operations: To do Gaussian and Gauss-Jordan elimination, we use three basic operations to transform the augmented matrix into another augmented matrix that represents an equivalent linear system – equivalent in the sense that the same values of \\(x_j\\) solve both the original and transformed matrix/system: Interchanging Rows: Suppose we have the augmented matrix \\[{\\widehat{\\bf A}}=\\begin{pmatrix} a_{11} &amp; a_{12} &amp; | &amp; b_1\\\\ a_{21} &amp; a_{22} &amp; | &amp; b_2 \\end{pmatrix}\\] If we interchange the two rows, we get the augmented matrix \\[\\begin{pmatrix} a_{21} &amp; a_{22} &amp; | &amp; b_2\\\\ a_{11} &amp; a_{12} &amp; | &amp; b_1 \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\bf A}\\). Multiplying by a Constant: If we multiply the second row of matrix \\(\\widehat{\\bf A}\\) by a constant \\(c\\), we get the augmented matrix \\[\\begin{pmatrix} a_{11} &amp; a_{12} &amp; | &amp; b_1\\\\ c a_{21} &amp; c a_{22} &amp; | &amp; c b_2 \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\bf A}\\). Adding (subtracting) Rows: If we add (subtract) the first row of matrix \\(\\widehat{\\bf A}\\) to the second, we obtain the augmented matrix \\[\\begin{pmatrix} a_{11} &amp; a_{12} &amp; | &amp; b_1\\\\ a_{11}+a_{21} &amp; a_{12}+a_{22} &amp; | &amp; b_1+b_2 \\end{pmatrix}\\] which represents a linear system equivalent to that represented by matrix \\(\\widehat{\\bf A}\\). Exercises: Using Gaussian or Gauss-Jordan elimination, solve the following linear systems by putting them into row echelon or reduced row echelon form: 2.9 Rank — and Whether a System Has One, Infinite, or No Solutions We previously noted that a \\(2\\times 2\\) system had one, infinite, or no solutions if the two lines intersected, were the same, or were parallel, respectively. More generally, to determine how many solutions exist, we can use information about (1) the number of equations \\(m\\), (2) the number of unknowns \\(n\\), and (3) the rank of the matrix representing the linear system. Rank: The row rank or column rank of a matrix is the number of nonzero rows or columns in its row echelon form. The rank also corresponds to the maximum number of linearly independent row or column vectors in the matrix. For any matrix A, the row rank always equals column rank, and we refer to this number as the rank of A. Examples: Let \\(\\bf A\\) be the coefficient matrix and \\(\\widehat{\\bf A}=[ {\\bf A} | {\\bf b}]\\) be the augmented matrix. Then Existence of Solutions: Find the rank and number of solutions for the systems of equations below.: 2.10 The Inverse of a Matrix Inverse Matrix: An \\(n\\times n\\) matrix \\({\\bf A}\\) is nonsingular or invertible if there exists an \\(n\\times n\\) matrix \\({\\bf A}^{-1}\\) such that \\[{\\bf A} {\\bf A}^{-1} = {\\bf A}^{-1} {\\bf A} = {\\bf I}_n\\] where \\({\\bf A}^{-1}\\) is the inverse of \\({\\bf A}\\). If there is no such \\({\\bf A}^{-1}\\), then \\({\\bf A}\\) is singular or noninvertible. Example: Let \\[{\\bf A} = \\begin{pmatrix} 2&amp;3\\\\2&amp;2 \\end{pmatrix}, \\qquad {\\bf B}=\\begin{pmatrix} -1&amp;\\frac{3}{2}\\\\ 1&amp;-1 \\end{pmatrix}\\] Since \\[{\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n\\] we conclude that \\({\\bf B}\\) is the inverse, \\({\\bf A}^{-1}\\), of \\({\\bf A}\\) and that \\({\\bf A}\\) is nonsingular. Properties of the Inverse: : We know that if \\({\\bf B}\\) is the inverse of \\({\\bf A}\\), then \\[{\\bf A} {\\bf B} = {\\bf B} {\\bf A} = {\\bf I}_n\\] Looking only at the first and last parts of this \\[{\\bf A} {\\bf B} = {\\bf I}_n\\] Solving for \\({\\bf B}\\) is equivalent to solving for \\(n\\) linear systems, where each column of \\({\\bf B}\\) is solved for the corresponding column in \\({\\bf I}_n\\). We can solve the systems simultaneously by augmenting \\({\\bf A}\\) with \\({\\bf I}_n\\) and performing Gauss-Jordan elimination on \\({\\bf A}\\). If Gauss-Jordan elimination on \\([{\\bf A} | {\\bf I}_n]\\) results in \\([{\\bf I}_n | {\\bf B} ]\\), then \\({\\bf B}\\) is the inverse of \\({\\bf A}\\). Otherwise, \\({\\bf A}\\) is singular.\\ [12pt] To summarize: To calculate the inverse of \\({\\bf A}\\) Exercise: Find the inverse of \\({\\bf A}=\\begin{pmatrix} 1&amp;1&amp;1\\\\0&amp;2&amp;3\\\\5&amp;5&amp;1 \\end{pmatrix}\\) 2.11 Linear Systems and Inverses Let’s return to the matrix representation of a linear system \\[\\bf{Ax} = \\bf{b}\\] If \\(\\bf{A}\\) is an \\(n\\times n\\) matrix,then \\(\\bf{Ax}=\\bf{b}\\) is a system of \\(n\\) equations in \\(n\\) unknowns. Suppose \\(\\bf{A}\\) is nonsingular. Then \\(\\bf{A}^{-1}\\) exists. To solve this system, we can premultiply each side by \\(\\bf{A}^{-1}\\) and reduce it as follows: \\[\\begin{eqnarray*} \\bf{A}^{-1} (\\bf{A} \\bf{x}) &amp; = &amp; \\bf{A}^{-1} \\bf{b} \\\\ (\\bf{A}^{-1} \\bf{A})\\bf{x} &amp; = &amp; \\bf{A}^{-1} \\bf{b}\\\\ \\bf{I}_n \\bf{x} &amp; = &amp; \\bf{A}^{-1} \\bf{b}\\\\ \\bf{x} &amp; = &amp; \\bf{A}^{-1} \\bf{b} \\end{eqnarray*}\\] Hence, given \\(\\bf{A}\\) and \\(\\bf{b}\\) and given that \\(\\bf{A}\\) is nonsingular, then \\(\\bf{x} = \\bf{A}^{-1} \\bf{b}\\) is a unique solution to this system. Notice also that the requirements for \\(\\bf{A}\\) to be nonsingular correspond to the requirements for a linear system to have a unique solution: \\[\\text{rank}\\bf{A} = \\text{rows}\\bf{A} = \\text{cols}\\bf{A}\\] 2.12 Determinants Singularity: Determinants can be used to whether a square matrix is nonsingular. A square matrix is nonsingular iff its determinant is not zero. Determinant of a \\(1 \\times 1\\) matrix, A, equals \\(a_{11}\\) Determinant of a \\(2 \\times 2\\) matrix, A, \\(\\begin{vmatrix} a_{11}&amp;a_{12}\\\\ a_{21}&amp;a_{22} \\end{vmatrix}\\): \\[\\begin{eqnarray*} \\det({\\bf A}) &amp;=&amp; |{\\bf A}|\\\\ &amp;=&amp; a_{11}|a_{22}| - a_{12}|a_{21}|\\\\ &amp;=&amp; a_{11}a_{22} - a_{12}a_{21} \\end{eqnarray*}\\] We can extend the second to last equation above to get the definition of the determinant of a \\(3 \\times 3\\) matrix: \\[\\begin{eqnarray*} \\begin{vmatrix} a_{11}&amp;a_{12}&amp;a_{13}\\\\ a_{21} &amp; a_{22}&amp;a_{23}\\\\ a_{31}&amp;a_{32}&amp;a_{33} \\end{vmatrix} &amp;=&amp; a_{11} \\begin{vmatrix} a_{22}&amp;a_{23}\\\\ a_{32}&amp;a_{33} \\end{vmatrix} - a_{12} \\begin{vmatrix} a_{21}&amp;a_{23}\\\\ a_{31}&amp;a_{33} \\end{vmatrix} + a_{13} \\begin{vmatrix} a_{21}&amp;a_{22}\\\\ a_{31}&amp;a_{32} \\end{vmatrix}\\\\ &amp;=&amp; a_{11}(a_{22}a_{33} - a_{23}a_{32}) - a_{12}(a_{21}a_{33} - a_{23}a_{31}) + a_{13}(a_{21}a_{32} - a_{22}a_{31}) \\end{eqnarray*}\\] Let’s extend this now to any \\(n\\times n\\) matrix. Let’s define \\({\\bf A}_{ij}\\) as the \\((n-1)\\times (n-1)\\) submatrix of \\({\\bf A}\\) obtained by deleting row \\(i\\) and column \\(j\\). Let the \\((i,j)\\)th minor of \\({\\bf A}\\) be the determinant of \\({\\bf A}_{ij}\\): \\[M_{ij}=|{\\bf A}_{ij}|\\] Then for any \\(n\\times n\\) matrix \\({\\bf A}\\) \\[|{\\bf A}|= a_{11}M_{11} - a_{12}M_{12} + \\cdots + (-1)^{n+1} a_{1n} M_{1n}\\] Example: Does the following matrix have an inverse? \\[{\\bf A}=\\begin{pmatrix} 1&amp;1&amp;1\\\\0&amp;2&amp;3\\\\5&amp;5&amp;1 \\end{pmatrix}\\] 1. Calculate its determinant. \\[\\begin{eqnarray} &amp;=&amp; 1(2-15) - 1(0-15) + 1(0-10) \\nonumber\\\\ &amp;=&amp; -13+15-10 \\nonumber\\\\ &amp;=&amp; -8\\nonumber \\end{eqnarray}\\] Since \\(|{\\bf A}|\\ne 0\\), we conclude that \\({\\bf A}\\) has an inverse. Determinant of Triangular or Diagonal Matrices: For any upper-triangular, lower-triangular, or diagonal matrix, the determinant is just the product of the diagonal terms. Example: Suppose we have the following square matrix in row echelon form (i.e., upper triangular) \\[{\\bf R} =\\begin{pmatrix} r_{11}&amp;r_{12}&amp;r_{13}\\\\ 0&amp;r_{22}&amp;r_{23}\\\\ 0&amp; 0&amp;r_{33} \\end{pmatrix}\\] Then \\[|{\\bf R}| = r_{11} \\begin{vmatrix} r_{22}&amp;r_{23}\\\\ 0&amp;r_{33} \\end{vmatrix} = r_{11}r_{22}r_{33}\\] Properties of Determinants: 2.13 Getting Inverse of a Matrix using its Determinant and Matrix of Cofactors Thus far, we have a number of algorithms to Find the solution of a linear system, Find the inverse of a matrix but these remain just that — algorithms. At this point, we have no way of telling how the solutions \\(x_j\\) change as the parameters \\(a_{ij}\\) and \\(b_i\\) change, except by changing the values and “rerunning” the algorithms. With determinants, we can 1. Provide an explicit formula for the inverse, and 2. Provide an explicit formula for the solution of an \\(n\\times n\\) linear system. Hence, we can examine how changes in the parameters and \\(b_i\\) affect the solutions \\(x_j\\). Determinant Formula for the Inverse of a \\(2 \\times 2\\): The determinant of a \\(2 \\times 2\\) matrix A \\(\\begin{pmatrix} a &amp; b\\\\ c &amp; d\\\\ \\end{pmatrix}\\) is defined as: \\[\\frac{1}{\\det({\\bf A})} \\begin{pmatrix} d &amp; -b\\\\ -c &amp; a\\\\ \\end{pmatrix}\\] 2.14 Inverse of Larger Matrices Cofactors and Adjoint Matrices: First define the \\((i,j)\\)th cofactor \\(C_{ij}\\) of \\({\\bf A}\\) as \\((-1)^{i+j}M_{ij}\\). Recall that \\(M_{ij}\\) is the minor of A, defined as the determinant of the matrix that results from removing row \\(i\\) and column \\(j\\) from A. Then define the adjoint of \\({\\bf A}\\) as the \\(n\\times n\\) matrix whose \\((i,j)\\)th entry is \\(C_{ji}\\) (notice the switch in indices!). In other words, adj(\\({\\bf A}\\)) is the transpose of the cofactor matrix of \\({\\bf A}\\). Then the inverse of \\({\\bf A}\\) is defined as the reciprocal of the determinant of A times its adjoint, given by the formula \\[{\\bf A}^{-1}=\\frac{1}{|{\\bf A}|}\\mbox{adj\\,}{\\bf A}= \\begin{pmatrix} \\frac{C_{11}}{|{\\bf A}|}&amp;\\frac{C_{21}}{|{\\bf A}|}&amp;\\cdots&amp;\\frac{C_{n1}}{|{\\bf A}|}\\\\[6pt] \\frac{C_{12}}{|{\\bf A}|}&amp;\\frac{C_{22}}{|{\\bf A}|}&amp;\\cdots&amp;\\frac{C_{n2}}{|{\\bf A}|}\\\\[6pt] \\vdots &amp;\\vdots &amp;\\ddots&amp;\\vdots \\\\[6pt] \\frac{C_{1n}}{|{\\bf A}|}&amp;\\frac{C_{2n}}{|{\\bf A}|}&amp;\\cdots&amp;\\frac{C_{nn}}{|{\\bf A}|} \\end{pmatrix}\\] Exercises: Let \\({\\bf A}=\\begin{pmatrix} 5&amp;1&amp;3\\\\0&amp;2&amp;3\\\\5&amp;5&amp;1 \\end{pmatrix}\\) Find the determinant of A. \\[\\begin{eqnarray*} \\det{\\bf A} &amp;=&amp; \\phantom{5\\cdot(2\\cdot 1 - 3\\cdot 5) - 1\\cdot(0\\cdot 1 - 3\\cdot 5) + 3\\cdot(2\\cdot 5 - 0\\cdot 5)}\\\\ &amp;=&amp; \\phantom{5\\cdot(-13) - (-15) + 3\\cdot(-10)}\\\\ &amp;=&amp; \\phantom{-65 + 15 - 30}\\\\ &amp;=&amp; \\phantom{-80} \\end{eqnarray*}\\] Find the adjoint matrix of A. adj({}) = Find the inverse of {}. \\({\\bf A}^{-1}\\) = "],
["functions-and-notation.html", "Chapter 3 Functions and Notation 3.1 Dimensionality 3.2 Interval Notation for \\({\\bf R}^1\\) 3.3 Neighborhoods: Intervals, Disks, and Balls 3.4 Introduction to Functions 3.5 Domain and Range/Image 3.6 Some General Types of Functions 3.7 \\(\\log\\), \\(\\ln\\), and \\(\\exp\\) 3.8 Other Useful Functions 3.9 Graphing Functions 3.10 Solving for Variables and Finding Inverses 3.11 Finding the Roots or Zeroes of a Function 3.12 The Limit of a Function 3.13 Continuity 3.14 Sets", " Chapter 3 Functions and Notation Topics Dimensionality; Interval Notation for \\({\\bf R}^1\\); Neighborhoods: Intervals, Disks, and Balls; Introduction to Functions; Domain and Range; Some General Types of Functions; \\(\\log\\), \\(\\ln\\), and \\(\\exp\\); Other Useful Functions; Graphing Functions; Solving for Variables; Finding Roots; Limit of a Function; Continuity; Sets, Sets, and More Sets. Much of the material and examples for this lecture are taken from Simon &amp; Blume (1994) , Boyce &amp; Diprima (1988) , and Protter &amp; Morrey (1991) 3.1 Dimensionality \\({\\bf R}^1\\) is the set of all real numbers extending from \\(-\\infty\\) to \\(+\\infty\\) — i.e., the real number line. \\({\\bf R}^n\\) is an \\(n\\)-dimensional space (often referred to as Euclidean space), where each of the \\(n\\) axes extends from \\(-\\infty\\) to \\(+\\infty\\). \\({\\bf R}^1\\) is a one dimensional line. \\({\\bf R}^2\\) is a two dimensional plane. \\({\\bf R}^3\\) is a three dimensional space. \\({\\bf R}^4\\) could be 3-D plus time (or temperature, etc). Points in \\({\\bf R}^n\\) are ordered \\(n\\)-tuples, where each element of the \\(n\\)-tuple represents the coordinate along that dimension. \\({\\bf R}^1\\): (3) \\({\\bf R}^2\\): (-15, 5) \\({\\bf R}^3\\): (86, 4, 0) 3.2 Interval Notation for \\({\\bf R}^1\\) Open interval: \\[(a,b)\\equiv \\{ x\\in{\\bf R}^1: a&lt;x&lt;b\\}\\] \\(x\\) is a one-dimensional element in which x is greater than a and less than b Closed interval: \\[[a,b]\\equiv \\{ x\\in{\\bf R}^1: a\\le x \\le b\\}\\] \\(x\\) is a one-dimensional element in which x is greater or equal to than a and less than or equal to b Half open, half closed: \\[(a,b]\\equiv \\{ x\\in{\\bf R}^1: a&lt;x\\le b\\}\\] \\(x\\) is a one-dimensional element in which x is greater than a and less than or equal to b 3.3 Neighborhoods: Intervals, Disks, and Balls In many areas of math, we need a formal construct for what it means to be “near” a point \\(\\bf c\\) in \\({\\bf R}^n\\). This is generally called the neighborhood of \\(\\bf c\\). It’s represented by an open interval, disk, or ball, depending on whether \\({\\bf R}^n\\) is of one, two, or more dimensions, respectively. Given the point \\(c\\), these are defined as \\(\\epsilon\\)-interval in \\({\\bf R}^1\\): \\(\\{x : |x-c|&lt;\\epsilon \\}\\). \\(x\\) is in the neighborhood of {} if it is in the open interval \\((c-\\epsilon,c+\\epsilon)\\). \\(\\epsilon\\)-disk in \\({\\bf R}^2\\): \\(\\{x : || x-c ||&lt;\\epsilon\\}\\). \\(x\\) is in the neighborhood of {} if it is inside the circle or disc with center \\(\\bf c\\) and radius \\(\\epsilon\\). \\(\\epsilon\\)-ball in \\({\\bf R}^n\\): \\(\\{x : || x-c ||&lt;\\epsilon\\}\\). \\(x\\) is in the neighborhood of {} if it is inside the sphere or ball with center \\(\\bf c\\) and radius \\(\\epsilon\\). 3.4 Introduction to Functions A function (in \\({\\bf R}^1\\)) is a mapping, or transformation, that relates members of one set to members of another set. For instance, if you have two sets: set \\(A\\) and set \\(B\\), a function from \\(A\\) to \\(B\\) maps every value \\(a\\) in set \\(A\\) such that \\(f(a) \\in B\\). Functions can be “many-to-one”, where many values or combinations of values from set \\(A\\) produce a single output in set \\(B\\), or they can be “one-to-one”, where each value in set \\(A\\) corresponds to a single value in set \\(B\\). Examples: Mapping notation Function of one variable: \\(f:{\\bf R}^1\\to{\\bf R}^1\\)\\ \\(f(x)=x+1\\). For each \\(x\\) in \\({\\bf R}^1\\), \\(f(x)\\) assigns the number \\(x+1\\). Function of two variables: \\(f: {\\bf R}^2\\to{\\bf R}^1\\). \\(f(x,y)=x^2+y^2\\). For each ordered pair \\((x,y)\\) in \\({\\bf R}^2\\), \\(f(x,y)\\) assigns the number \\(x^2+y^2\\). We often use variable \\(x\\) as input and another \\(y\\) as output, e.g. \\(y=x+1\\) 3.5 Domain and Range/Image Some functions are defined only on proper subsets of \\({\\bf R}^n\\). Domain: the set of numbers in \\(X\\) at which \\(f(x)\\) is defined. Range: elements of \\(Y\\) assigned by \\(f(x)\\) to elements of \\(X\\), or \\[f(X)=\\{ y : y=f(x), x\\in X\\}\\] Most often used when talking about a function \\(f:{\\bf R}^1\\to{\\bf R}^1\\). Image: same as range, but more often used when talking about a function \\(f:{\\bf R}^n\\to{\\bf R}^1\\). 3.6 Some General Types of Functions Monomials: \\(f(x)=a x^k\\)\\ \\(a\\) is the coefficient. \\(k\\) is the degree.\\ Examples: \\(y=x^2\\), \\(y=-\\frac{1}{2}x^3\\) Polynomials: sum of monomials. Examples: \\(y=-\\frac{1}{2}x^3+x^2\\), \\(y=3x+5\\) The degree of a polynomial is the highest degree of its monomial terms. Also, it’s often a good idea to write polynomials with terms in decreasing degree. Rational Functions: ratio of two polynomials. Examples: \\(y=\\frac{x}{2}\\), \\(y=\\frac{x^2+1}{x^2-2x+1}\\) Exponential Functions: Example: \\(y=2^x\\) Trigonometric Functions: Examples: \\(y=\\cos(x)\\), \\(y=3\\sin(4x)\\) 3.7 \\(\\log\\), \\(\\ln\\), and \\(\\exp\\) Relationship of logarithmic and exponential functions: \\[y=\\log_a(x) \\iff a^y=x\\] The log function can be thought of as an inverse for exponential functions. \\(a\\) is referred to as the “base” of the logarithm. Common Bases: The two most common logarithms are base 10 and base \\(e\\). Base 10: \\(\\quad y=\\log_{10}(x) \\iff 10^y=x\\). The base 10 logarithm is often simply written as “\\(\\log(x)\\)” with no base denoted. Base \\(e\\): \\(\\quad y=\\log_e(x) \\iff e^y=x\\). The base \\(e\\) logarithm is referred to as the “natural” logarithm and is written as ``\\(\\ln(x)\\)“. Properties of exponential functions: \\(a^x a^y = a^{x+y}\\) \\(a^{-x} = 1/a^x\\) \\(a^x/a^y = a^{x-y}\\) \\((a^x)^y = a^{x y}\\) \\(a^0 = 1\\) Properties of logarithmic functions (any base): Generally, when statisticians or social scientists write \\(\\log(x)\\) they mean \\(\\log_e(x)\\). In other words: \\(\\log_e(x) \\equiv \\ln(x) \\equiv \\log(x)\\) \\[\\log_a(a^x)=x\\] and \\[a^{\\log_a(x)}=x\\] \\(\\log(x y)=\\log(x)+\\log(y)\\) \\(\\log(x^y)=y\\log(x)\\) \\(\\log(1/x)=\\log(x^{-1})=-\\log(x)\\) \\(\\log(x/y)=\\log(x\\cdot y^{-1})=\\log(x)+\\log(y^{-1})=\\log(x)-\\log(y)\\) \\(\\log(1)=\\log(e^0)=0\\) Change of Base Formula: Use the change of base formula to switch bases as necessary: \\[\\log_b(x) = \\frac{\\log_a(x)}{\\log_a(b)}\\] Example: \\[\\log_{10}(x) = \\frac{\\ln(x)}{\\ln(10)}\\] 3.8 Other Useful Functions Factorials!: \\[x! = x\\cdot (x-1) \\cdot (x-2) \\cdots (1)\\] Modulo: Tells you the remainder when you divide one number by another. Can be extremely useful for programming: or \\(17 \\mod 3 = 2\\) \\(100 \\ \\% \\ 30 = 10\\) Summation: \\[\\sum\\limits_{i=1}^n x_i = x_1+x_2+x_3+\\cdots+x_n\\] Product: \\[\\prod\\limits_{i=1}^n x_i = x_1 x_2 x_3 \\cdots x_n\\] Properties: You can use logs to go between sum and product notation. This will be particularly important when you’re learning maximum likelihood estimation. \\[\\begin{eqnarray*} \\log \\bigg(\\prod\\limits_{i=1}^n x_i \\bigg) &amp;=&amp; \\log(x_1 \\cdot x_2 \\cdot x_3 \\cdots \\cdot x_n)\\\\ &amp;=&amp; \\log(x_1) + \\log(x_2) + \\log(x_3) + \\cdots + \\log(x_n)\\\\ &amp;=&amp; \\sum\\limits_{i=1}^n \\log (x_i) \\end{eqnarray*}\\] Therefore, you can see that the log of a product is equal to the sum of the logs. We can write this more generally by adding in a constant, \\(c\\): \\[\\begin{eqnarray*} \\log \\bigg(\\prod\\limits_{i=1}^n c x_i\\bigg) &amp;=&amp; \\log(cx_1 \\cdot cx_2 \\cdots cx_n)\\\\ &amp;=&amp; \\log(c^n \\cdot x_1 \\cdot x_2 \\cdots x_n)\\\\ &amp;=&amp; \\log(c^n) + \\log(x_1) + \\log(x_2) + \\cdots + \\log(x_n)\\\\\\\\ &amp;=&amp; n \\log(c) + \\sum\\limits_{i=1}^n \\log (x_i)\\\\ \\end{eqnarray*}\\] 3.9 Graphing Functions What can a graph tell you about a function? Is the function increasing or decreasing? Over what part of the domain? How ``fast&quot; does it increase or decrease? Are there global or local maxima and minima? Where? Are there inflection points? Is the function continuous? Is the function differentiable? Does the function tend to some limit? Other questions related to the substance of the problem at hand. 3.10 Solving for Variables and Finding Inverses Sometimes we’re given a function \\(y=f(x)\\) and we want to find how \\(x\\) varies as a function of \\(y\\). If \\(f\\) is a one-to-one mapping, then it has an inverse. Use algebra to move \\(x\\) to the left hand side (LHS) of the equation and so that the right hand side (RHS) is only a function of \\(y\\).\\ Examples: (we want to solve for \\(x\\)) \\[y=3x+2 \\quad\\Longrightarrow\\quad -3x=2-y \\quad\\Longrightarrow\\quad 3x=y-2 \\quad\\Longrightarrow\\quad x=\\frac{1}{3}(y-2)\\] \\[y=3x-4z+2 \\quad \\Longrightarrow\\quad y+4z-2=3x \\quad\\Longrightarrow\\quad x=\\frac{1}{3}(y+4z-2)\\] \\[\\begin{align*} y=e^x+4 &amp;\\Longrightarrow\\quad y-4=e^x\\\\ &amp;\\Longrightarrow \\ln(y-4) = \\ln(e^x)\\\\ &amp;\\Longrightarrow\\quad x=\\ln(y-4) \\end{align*}\\] Sometimes (often?) the inverse does not exist. For example, we’re given the function \\(y=x^2\\) (a parabola). Solving for \\(x\\), we get \\(x=\\pm\\sqrt{y}\\). For each value of \\(y\\), there are two values of \\(x\\). 3.11 Finding the Roots or Zeroes of a Function Solving for variables is especially important when we want to find the roots of an equation: those values of variables that cause an equation to equal zero. Especially important in finding equilibria and in doing maximum likelihood estimation. Procedure: Given \\(y=f(x)\\), set \\(f(x)=0\\). Solve for \\(x\\). Multiple Roots: \\[f(x)=x^2 - 9 \\quad\\Longrightarrow\\quad 0=x^2 - 9 \\quad\\Longrightarrow\\quad 9=x^2 \\quad\\Longrightarrow\\quad \\pm \\sqrt{9}=\\sqrt{x^2} \\quad\\Longrightarrow\\quad \\pm 3=x\\] Quadratic Formula: For quadratic equations \\(ax^2+bx+c=0\\), use the quadratic formula: \\[x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\] Examples: 3.12 The Limit of a Function We’re often interested in determining if a function \\(f\\) approaches some number \\(L\\) as its independent variable \\(x\\) moves to some number \\(c\\) (usually 0 or \\(\\pm\\infty\\)). If it does, we say that the limit of \\(f(x)\\), as \\(x\\) approaches \\(c\\), is \\(L\\): \\(\\lim\\limits_{x \\to c} f(x)=L\\). For a limit \\(L\\) to exist, the function \\(f(x)\\) must approach \\(L\\) from both the left and right. Limit of a function. Let \\(f(x)\\) be defined at each point in some open interval containing the point \\(c\\). Then \\(L\\) equals \\(\\lim\\limits_{x \\to c} f(x)\\) if for any (small positive) number \\(\\epsilon\\), there exists a corresponding number \\(\\delta&gt;0\\) such that if \\(0&lt;|x-c|&lt;\\delta\\), then \\(|f(x)-L|&lt;\\epsilon\\). Note: f(x) does not necessarily have to be defined at \\(c\\) for \\(\\lim\\limits_{x \\to c}\\) to exist. Uniqueness: \\(\\lim\\limits_{x \\to c} f(x)=L\\) and \\(\\lim\\limits_{x \\to c} f(x)=M \\Longrightarrow L=M\\) Properties: Let \\(f\\) and \\(g\\) be functions with \\(\\lim\\limits_{x \\to c} f(x)=A\\) and \\(\\lim\\limits_{x \\to c} g(x)=B\\). \\(\\lim\\limits_{x \\to c}[f(x)+g(x)]=\\lim\\limits_{x \\to c} f(x)+ \\lim\\limits_{x \\to c} g(x)\\) \\(\\lim\\limits_{x \\to c} \\alpha f(x) = \\alpha \\lim\\limits_{x \\to c} f(x)\\) \\(\\lim\\limits_{x \\to c} f(x) g(x) = [\\lim\\limits_{x \\to c} f(x)][\\lim\\limits_{x \\to c} g(x)]\\) \\(\\lim\\limits_{x \\to c} \\frac{f(x)}{g(x)} = \\frac{\\lim\\limits_{x \\to c} f(x)}{\\lim\\limits_{x \\to c} g(x)}\\), provided \\(\\lim\\limits_{x \\to c} g(x)\\ne 0\\). Note: In a couple days we’ll talk about L’H^opital’s Rule, which uses simple calculus to help find the limits of functions like this. Examples: \\[\\begin{align*} &amp; \\lim_{x \\to c} k = \\\\ &amp; \\lim_{x \\to c} x = \\\\ &amp; \\lim_{x\\to 2} (2x-3) =\\\\ &amp; \\lim_{x \\to c} x^n = \\end{align*}\\] Types of limits: Right-hand limit: The value approached by \\(f(x)\\) when you move from right to left. Left-hand limit: The value approached by \\(f(x)\\) when you move from left to right. Infinity: The value approached by \\(f(x)\\) as x grows infinitely large. Sometimes this may be a number; sometimes it might be \\(\\infty\\) or \\(-\\infty\\). Negative infinity: The value approached by \\(f(x)\\) as x grows infinitely negative. Sometimes this may be a number; sometimes it might be \\(\\infty\\) or \\(-\\infty\\). 3.13 Continuity Continuity: Suppose that the domain of the function \\(f\\) includes an open interval containing the point \\(c\\). Then \\(f\\) is continuous at \\(c\\) if \\(\\lim\\limits_{x \\to c} f(x)\\) exists and if \\(\\lim\\limits_{x \\to c} f(x)=f(c)\\). Further, \\(f\\) is continuous on an open interval \\((a,b)\\) if it is continuous at each point in the interval. Examples: Continuous functions. Properties: If \\(f\\) and \\(g\\) are continuous at point \\(c\\), then \\(f+g\\), \\(f-g\\), \\(f \\cdot g\\), \\(|f|\\), and \\(\\alpha f\\) are continuous at point \\(c\\) also. \\(f/g\\) is continuous, provided \\(g(c)\\ne 0\\). Boundedness: If \\(f\\) is continuous on the closed bounded interval \\([a,b]\\), then there is a number \\(K\\) such that \\(|f(x)|\\le K\\) for each \\(x\\) in \\([a,b]\\). Max/Min: If \\(f\\) is continuous on the closed bounded interval \\([a,b]\\), then \\(f\\) has a maximum and a minimum on \\([a,b]\\). They may be located at the end points. 3.14 Sets Interior Point: The point \\(\\bf x\\) is an interior point of the set \\(S\\) if \\(\\bf x\\) is in \\(S\\) and if there is some \\(\\epsilon\\)-ball around \\(\\bf x\\) that contains only points in \\(S\\). The interior of \\(S\\) is the collection of all interior points in \\(S\\). The interior can also be defined as the union of all open sets in \\(S\\). If the set \\(S\\) is circular, the interior points are everything inside of the circle, but not on the circle’s rim. Example: The interior of the set \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) is \\(\\{ (x,y) : x^2+y^2&lt; 4 \\}\\) . Boundary Point: The point \\(\\bf x\\) is a boundary point of the set \\(S\\) if every \\(\\epsilon\\)-ball around \\(\\bf x\\) contains both points that are in \\(S\\) and points that are outside \\(S\\). The boundary is the collection of all boundary points. If the set \\(S\\) is circular, the boundary points are everything on the circle’s rim. Example: The boundary of \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) is \\(\\{ (x,y) : x^2+y^2 = 4 \\}\\). Open: A set \\(S\\) is open if for each point \\(\\bf x\\) in \\(S\\), there exists an open \\(\\epsilon\\)-ball around \\(\\bf x\\) completely contained in \\(S\\). If the set \\(S\\) is circular and open, the points contained within the set get infinitely close to the circle’s rim, but do not touch it. Example: \\(\\{ (x,y) : x^2+y^2&lt;4 \\}\\) Closed: A set \\(S\\) is closed if it contains all of its boundary points. If the set \\(S\\) is circular and closed, the set contains all points within the rim as well as the rim itself. Example: \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) Note: a set may be neither open nor closed. Example: \\(\\{ (x,y) : 2 &lt; x^2+y^2\\le 4 \\}\\) Complement: The complement of set \\(S\\) is everything outside of \\(S\\). If the set \\(S\\) is circular, the complement of \\(S\\) is everything outside of the circle. Example: The complement of \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) is \\(\\{ (x,y) : x^2+y^2 &gt; 4 \\}\\). Closure: The closure of set \\(S\\) is the smallest closed set that contains \\(S\\). Example: The closure of \\(\\{ (x,y) : x^2+y^2&lt;4 \\}\\) is \\(\\{ (x,y) : x^2+y^2\\le 4 \\}\\) Bounded: A set \\(S\\) is bounded if it can be contained within an \\(\\epsilon\\)-ball. Examples: Bounded: any interval that doesn’t have \\(\\infty\\) or \\(-\\infty\\) as endpoints; any disk in a plane with finite radius. Unbounded: the set of integers in \\({\\bf R}^1\\); any ray. Compact: A set is compact if and only if it is both closed and bounded. Empty: The empty (or null) set is a unique set that has no elements, denoted by {} or \\(\\o\\). Examples: The set of squares with 5 sides; the set of countries south of the South Pole. The set, \\(S\\), denoted by \\(\\{\\o\\}\\) is technically empty. That is because this set contains the empty set within it, so \\(S\\) is not empty. "],
["calculus-i.html", "Chapter 4 Calculus I 4.1 Sequences 4.2 The Limit of a Sequence 4.3 Series 4.4 Derivatives 4.5 Higher-Order Derivatives or, Derivatives of Derivatives of Derivatives 4.6 Composite Functions and the Chain Rule 4.7 Derivatives of Euler’s number and natural logs 4.8 Applications of the Derivative: Maxima and Minima 4.9 Partial Derivatives 4.10 L’H^opital’s Rule 4.11 Taylor Series Approximation 4.12 Summary: Derivative calculus in 6 steps", " Chapter 4 Calculus I Topics: Sequences; Limit of a Sequence; Series; Derivatives; Higher-Order Derivatives; Composite Functions and The Chain Rule; Derivatives of Exp and Ln; Maxima and Minima; Partial Derivatives; L’H^opital’s Rule; Taylor Approximation; Derivative Calculus in 6 Steps Much of the material and examples for this lecture are taken from Simon &amp; Blume (1994) and from Boyce &amp; Diprima (1988) 4.1 Sequences A \\(\\{y_n\\}=\\{y_1, y_2, y_3, \\ldots, y_n\\}\\) is an ordered set of real numbers, where \\(y_1\\) is the first term in the sequence and \\(y_n\\) is the \\(n\\)th term. Generally, a sequence is , that is it extends to \\(n=\\infty\\). We can also write the sequence as \\(\\{y_n\\}^\\infty_{n=1}\\). Examples: Think of sequences like functions. Before, we had \\(y=f(x)\\) with \\(x\\) specified over some domain. Now we have \\(\\{y_n\\}=\\{f(n)\\}\\) with \\(n=1,2,3,\\ldots\\). Three kinds of sequences: Sequences like 1 above that converge to a limit. Sequences like 2 above that increase without bound. Sequences like 3 above that neither converge nor increase without bound — alternating over the number line. Boundedness and monotonicity: : if \\(|y_n|\\le K\\) for all \\(n\\) : \\(y_{n+1}&gt;y_n\\) for all \\(n\\) : \\(y_{n+1}&lt;y_n\\) for all \\(n\\) : Choose an infinite collection of entries from \\(\\{ y_n \\}\\), retaining their order. 4.2 The Limit of a Sequence We’re often interested in whether a sequence to a . Limits of sequences are conceptually similar to the limits of functions addressed in the previous lecture. . The sequence \\(\\{y_n\\}\\) has the limit \\(L\\), that is \\(\\lim\\limits_{n \\to \\infty} y_n =L\\), if for any \\(\\epsilon&gt;0\\) there is an integer \\(N\\) (which depends on \\(\\epsilon\\)) with the property that \\(|y_n -L|&lt;\\epsilon\\) for each \\(n&gt;N\\). \\(\\{y_n\\}\\) is said to converge to \\(L\\). If the above does not hold, then \\(\\{y_n\\}\\) diverges. : If \\(\\{y_n\\}\\) converges, then the limit \\(L\\) is unique. Let \\(\\lim\\limits_{n \\to \\infty} y_n = A\\) and \\(\\lim\\limits_{n \\to \\infty} z_n =B\\). Then Examples Finding the limit of a sequence in \\({\\bf R}^n\\) is similar to that in \\({\\bf R}^1\\). The sequence of vectors \\(\\bf \\{ y_n \\}\\) has the limit \\(\\bf L\\), that is \\(\\lim\\limits_{n\\to\\infty} {\\bf y_n}={\\bf L}\\), if for any \\(\\epsilon\\) there is an integer \\(N\\) where \\(\\bf ||y_n-L||&lt;\\epsilon\\) for each \\(n&gt;N\\). The sequence of vectors \\(\\bf \\{ y_n\\}\\) is said to converge to the vector \\(\\bf L\\) — and the distances between \\(\\bf y_n\\) and \\(\\bf L\\) converge to zero. Think of each coordinate of the vector \\(\\bf y_n\\) as being part of its own sequence over \\(n\\). Then a sequence of vectors in \\({\\bf R}^n\\) converges if and only if all \\(n\\) sequences of its components converge. Examples: : Any sequence contained in a compact (i.e., closed and bounded) subset of \\({\\bf R}^n\\) contains a convergent subsequence.\\ 4.3 Series The sum of the terms of a sequence is a . As there are both finite and infinite sequences, there are and . The series associated with the sequence \\(\\{y_n\\}=\\{y_1, y_2, y_3, \\ldots, y_n\\} = \\{y_n\\}_{n=1}^{\\infty}\\) is \\(\\sum_{n=1}^{\\infty} y_n\\). The \\(n\\)th partial sum \\(S_n\\) is defined as \\(S_n=\\sum_{k=1}^n y_k\\),the sum of the first \\(n\\) terms of the sequence. A series \\(\\sum y_n\\) converges if the sequence of partial sums \\(\\{S_1, S_2, S_3, ...\\}\\) converges, i.e has a finite limit. A is a series that can be written as \\(\\sum_{n=0}^{\\infty} r^n\\), where \\(r\\) is called the ratio. A geometric series converges to \\(\\frac{1}{1-r}\\) if \\(|r|&lt; 1\\) and diverges otherwise. For example, \\(\\sum_{n=0}^{\\infty} \\frac{1}{2^n} = 2\\). Examples of other series: 4.4 Derivatives The derivative of \\(f\\) at \\(x\\) is its rate of change at \\(x\\) — i.e., how much \\(f(x)\\) changes with a change in \\(x\\). For a line, the derivative is the slope.\\ For a curve, the derivative is the slope of the line tangent to the curve at \\(x\\). : Let \\(f\\) be a function whose domain includes an open interval containing the point \\(x\\). The derivative of \\(f\\) at \\(x\\) is given by \\[\\begin{eqnarray} f&#39;(x) &amp;=&amp;\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x}\\nonumber\\\\ &amp;=&amp;\\lim\\limits_{h\\to 0} \\frac{f(x+h)-f(x)}{h}\\nonumber \\end{eqnarray}\\] If \\(f&#39;(x)\\) exists at a point \\(x\\), then \\(f\\) is said to be differentiable at \\(x\\). Similarly, if \\(f&#39;(x)\\) exists for every point a long an interval, then \\(f\\) is differentiable along that interval. For \\(f\\) to be differentiable at \\(x\\), \\(f\\) must be both continuous and “smooth’’ at \\(x\\). The process of calculating \\(f&#39;(x)\\) is called differentiation. Notation for derivatives: Suppose that \\(f\\) and \\(g\\) are differentiable at \\(x\\) and that \\(\\alpha\\) is a constant. Then the functions \\(f\\pm g\\), \\(\\alpha f\\), \\(f g\\), and \\(f/g\\) (provided \\(g(x)\\ne 0\\)) are also differentiable at \\(x\\). Additionally, 4.5 Higher-Order Derivatives or, Derivatives of Derivatives of Derivatives We can keep applying the differentiation process to functions that are themselves derivatives. The derivative of \\(f&#39;(x)\\) with respect to \\(x\\), would then be \\[f&#39;&#39;(x)=\\lim\\limits_{h\\to 0}\\frac{f&#39;(x+h)-f&#39;(x)}{h}\\] and so on. Similarly, the derivative of \\(f&#39;&#39;(x)\\) would be denoted \\(f&#39;&#39;&#39;(x)\\). \\(f&#39;(x)\\), \\(y&#39;\\), \\(\\frac{df(x)}{dx}\\), \\(\\frac{dy}{dx}\\) \\(f&#39;&#39;(x)\\), \\(y&#39;&#39;\\), \\(\\frac{d^2f(x)}{dx^2}\\), \\(\\frac{d^2y}{dx^2}\\) \\(\\frac{d^nf(x)}{dx^n}\\), \\(\\frac{d^ny}{dx^n}\\) Example: \\[\\begin{align*} f(x) &amp;=x^3 f&#39;(x) &amp;=3x^2 f&#39;&#39;(x) &amp;=6x f&#39;&#39;&#39;(x) &amp;=6 f&#39;&#39;&#39;&#39;(x) &amp;=0 \\end{align*}\\] 4.6 Composite Functions and the Chain Rule Composite functions are formed by substituting one function into another and are denoted by \\[(f\\circ g)(x)=f[g(x)]\\] To form \\(f[g(x)]\\), the range of \\(g\\) must be contained (at least in part) within the domain of \\(f\\). The domain of \\(f\\circ g\\) consists of all the points in the domain of \\(g\\) for which \\(g(x)\\) is in the domain of \\(f\\). Examples: : Let \\(y=(f\\circ g)(x)= f[g(x)]\\). The derivative of \\(y\\) with respect to \\(x\\) is \\[\\frac{d}{dx} \\{ f[g(x)] \\} = f&#39;[g(x)] g&#39;(x)\\] which can also be written as \\[\\frac{dy}{dx}=\\frac{dy}{dg(x)} \\frac{dg(x)}{dx}\\] (Note: the above does not imply that the \\(dg(x)\\)’s cancel out, as in fractions. They are part of the derivative notation and you can’t separate them out or cancel them.) The chain rule can be thought of as the derivative of the “outside” times the derivative of the “inside”, remembering that the derivative of the outside function is evaluated at the value of the inside function. : If \\(y=[g(x)]^k\\), then \\(dy/dx=k[g(x)]^{k-1}g&#39;(x)\\). 4.7 Derivatives of Euler’s number and natural logs Derivatives of Exp or \\(e\\): Examples: Find \\(dy/dx\\) for : Examples: Find \\(dy/dx\\) for 4.8 Applications of the Derivative: Maxima and Minima The first derivative, \\(f&#39;(x)\\), identifies whether the function \\(f(x)\\) at the point \\(x\\) is increasing or decreasing at \\(x\\). Examples: The second derivative \\(f&#39;&#39;(x)\\) identifies whether the function \\(f(x)\\) at the point \\(x\\) is Maximum (Minimum): \\(x_0\\) is a local maximum (minimum) if \\(f(x_0)&gt;f(x)\\) (\\(f(x_0)&lt;f(x))\\) for all \\(x\\) within some open interval containing \\(x_0\\). \\(x_0\\) is a __ global maximum (minimum)__ if \\(f(x_0)&gt;f(x)\\) (\\(f(x_0)&lt;f(x))\\) for all \\(x\\) in the domain of \\(f\\). Critical points: Given the function \\(f\\) defined over domain \\(D\\), all of the following are defined as critical points: Any interior point of \\(D\\) where \\(f&#39;(x)=0\\). Any interior point of \\(D\\) where \\(f&#39;(x)\\) does not exist. Any endpoint that is in \\(D\\). The maxima and minima will be a subset of the critical points. Second Derivative Test of Maxima/Minima: We can use the second derivative to tell us whether a point is a maximum or minimum of \\(f(x)\\). . Sometimes no global max or min exists — e.g., \\(f(x)\\) not bounded above or below. However, there are three situations where we can fairly easily identify global max or min. Examples: Find any critical points and identify whether they’re a max, min, or saddle point: 4.9 Partial Derivatives Suppose we have a function \\(f\\) now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find it’s partial derivative, which is defined similar to the derivative of a function of one variable. Partial Derivative: Let \\(f\\) be a function of the variables \\((x_1,\\ldots,x_n)\\). The partial derivative of \\(f\\) with respect to \\(x_i\\) is \\[\\frac{\\partial f}{\\partial x_i} (x_1,\\ldots,x_n) = \\lim\\limits_{h\\to 0} \\frac{f(x_1,\\ldots,x_i+h,\\ldots,x_n)-f(x_1,\\ldots,x_i,\\ldots,x_n)}{h}\\] Only the \\(i\\)th variable changes — the others are treated as constants. We can take higher-order partial derivatives, like we did with functions of a single variable, except now we the higher-order partials can be with respect to multiple variables. 4.10 L’H^opital’s Rule In studying limits, we saw that \\(\\lim\\limits_{x \\to c} f(x)/g(x) = \\left(\\lim\\limits_{x \\to c} f(x)\\right)/\\left(\\lim\\limits_{x \\to c} g(x)\\right)\\), provided that \\(\\lim\\limits_{x \\to c} g(x)\\ne 0\\). If both \\(\\lim\\limits_{x \\to c} f(x)=0\\) and \\(\\lim\\limits_{x \\to c} g(x)=0\\), then we get an indeterminate form of the type \\(0/0\\) as \\(x\\to c\\). However, a limit may still exist. We can use L’H^opital’s rule to find the limit. L’H^opital’s Rule: Suppose \\(f\\) and \\(g\\) are differentiable on some interval \\(a&lt;x&lt;b\\) and that either Suppose further that \\(g&#39;(x)\\) is never zero on \\(a&lt;x&lt;b\\) and that \\[\\lim\\limits_{x\\to a^+} \\frac{f&#39;(x)}{g&#39;(x)}=L\\] then \\[\\lim\\limits_{x\\to a^+} \\frac{f(x)}{g(x)}=L\\] %Put more simply, if \\(\\lim\\limits_{x\\to a f(x)}\\) is of the form \\(0/0\\) or \\(\\pm \\infty / \\pm \\infty\\), then \\[\\lim\\limits_{x\\to a} f(x) = \\lim\\limits_{x\\to a} f&#39;(x)\\]\\ And if \\(\\lim\\limits_{x\\to a} \\frac{f&#39;(x)}{g&#39;(x)} = 0/0\\) or \\(\\pm \\infty / \\pm \\infty\\) then you can apply L’H^opital’s rule a second time, and continue applying it until you have a solution. 4.11 Taylor Series Approximation (also known as the delta method) are used commonly to represent functions as infinite series of the function’s derivatives at some point \\(a\\). For example, Taylor series are very helpful in representing nonlinear functions as linear functions. One can thus approximate functions by using lower-order, finite series known as . If \\(a=0\\), the series is called a Maclaurin series. Specifically, a Taylor series of a real or complex function \\(f(x)\\) that is infinitely differentiable in the neighborhood of point \\(a\\) is: \\[\\begin{align*} f(x) &amp;= f(a) + \\frac{f&#39;(a)}{1!} (x-a) + \\frac{f&#39;&#39;(a)}{2!} (x-a)^2 + \\cdots\\\\ &amp;= \\sum_{n=0}^\\infty \\frac{f^{(n)} (a)}{n!} (x-a)^n \\end{align*}\\] Taylor Approximation: We can often approximate the curvature of a function \\(f(x)\\) at point \\(a\\) using a 2nd order Taylor polynomial around point \\(a\\): \\[f(x) = f(a) + \\frac{f&#39;(a)}{1!} (x-a) + \\frac{f&#39;&#39;(a)}{2!} (x-a)^2 + R_2\\] \\(R_2\\) is the Lagrange remainder and often treated as negligible, giving us: \\[f(x) \\approx f(a) + f&#39;(a)(x-a) + \\dfrac{f&#39;&#39;(a)}{2} (x-a)^2\\] Taylor series expansion is easily generalized to multiple dimensions.\\ : The Hessian is used in a Taylor polynomial approximation to \\(\\fx\\) and provides information about the curvature of \\(\\fx\\) at \\(\\bm{x}\\) — e.g., which tells us whether a critical point \\(\\bm{x}^*\\) is a min, max, or saddle point. 4.12 Summary: Derivative calculus in 6 steps With these six rules (and decent algebra and trigonometry skills) you can figure out the derivative of anything. Sum rule: \\[[f(x)\\pm g(x)]&#39; = f&#39;(x)\\pm g&#39;(x)\\] Product rule: \\[[f(x)g(x)]&#39; = f&#39;(x)g(x)+f(x)g&#39;(x)\\] Power rule: \\[[x^k]&#39; = k x^{k-1}\\] Chain rule: \\[\\frac{d}{dx} \\{ f[g(x)] \\} = f&#39;[g(x)] g&#39;(x)\\] \\(e^x\\): \\[\\frac{d}{dx} e^x = e^x\\] Trig identity: \\[\\frac{d}{dx} \\sin(x) = \\cos(x)\\] "],
["calculus-ii.html", "Chapter 5 Calculus II 5.1 The Indefinite Integral: The Antiderivative 5.2 Common Rules of Integration 5.3 The Definite Integral: The Area under the Curve 5.4 Integration by Substitution 5.5 Integration by Parts, or Ultraviolet Voodoo", " Chapter 5 Calculus II Today’s Topics: The Indefinite Integral: The Antiderivative; Common Rules of Integration; The Definite Integral: The Area under the Curve; Integration by Substitution; Integration by Parts Much of the material and examples for this lecture are taken from Simon &amp; Blume (1994) and from Boyce &amp; Diprima (1988) 5.1 The Indefinite Integral: The Antiderivative So far, we’ve been interested in finding the derivative \\(g=f&#39;\\) of a function \\(f\\). However, sometimes we’re interested in exactly the reverse: finding the function \\(f\\) for which \\(g\\) is its derivative. We refer to \\(f\\) as the {} of \\(g\\). Let \\(DF\\) be the derivative of \\(F\\). And let \\(DF(x)\\) be the derivative of \\(F\\) evaluated at \\(x\\). Then the antiderivative is denoted by \\(D^{-1}\\) (i.e., the inverse derivative). If \\(DF=f\\), then \\(F=D^{-1}f\\). Indefinite Integral: Equivalently, if \\(F\\) is the antiderivative of \\(f\\), then \\(F\\) is also called the indefinite integral of \\(f\\) and written \\(F(x)=\\int\\limits f(x)dx\\). Notice from these examples that while there is only a single derivative for any function, there are multiple antiderivatives: one for any arbitrary constant \\(c\\). \\(c\\) just shifts the curve up or down on the \\(y\\)-axis. If more info is present about the antiderivative — e.g., that it passes through a particular point — then we can solve for a specific value of \\(c\\). \\end{itemize} 5.2 Common Rules of Integration 5.3 The Definite Integral: The Area under the Curve Riemann Sum: Suppose we want to determine the area \\(A(R)\\) of a region \\(R\\) defined by a curve \\(f(x)\\) and some interval \\(a\\le x \\le b\\). One way to calculate the area would be to divide the interval \\(a\\le x\\le b\\) into \\(n\\) subintervals of length \\(\\Delta x\\) and then approximate the region with a series of rectangles, where the base of each rectangle is \\(\\Delta x\\) and the height is \\(f(x)\\) at the midpoint of that interval. \\(A(R)\\) would then be approximated by the area of the union of the rectangles, which is given by \\[S(f,\\Delta x)=\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] and is called a Riemann sum. As we decrease the size of the subintervals \\(\\Delta x\\), making the rectangles “thinner,” we would expect our approximation of the area of the region to become closer to the true area. This gives the limiting process \\[A(R)=\\lim\\limits_{\\Delta x\\to 0}\\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] Riemann Integral: If for a given function \\(f\\) the Riemann sum approaches a limit as \\(\\Delta x \\to 0\\), then that limit is called the Riemann integral of \\(f\\) from \\(a\\) to \\(b\\). Formally, \\[\\int\\limits_a^b f(x) dx= \\lim\\limits_{\\Delta x\\to 0} \\sum\\limits_{i=1}^n f(x_i)\\Delta x\\] Definite Integral: We use the notation \\(\\int\\limits_a^b f(x) dx\\) to denote the definite integral of \\(f\\) from \\(a\\) to \\(b\\). In words, the definite integral \\(\\int\\limits_a^b f(x)dx\\) is the area under the ``curve&quot; f(x) from \\(x=a\\) to \\(x=b\\). First Fundamental Theorem of Calculus: Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Then the function \\[F(x)=\\int\\limits_a^x f(t)dt, \\quad a\\le x\\le b\\] has a derivative at each point in \\((a,b)\\) and \\[F&#39;(x)=f(x), \\quad a&lt;x&lt;b\\] This last point shows that differentiation is the inverse of integration. Second Fundamental Theorem of Calculus: Let the function \\(f\\) be bounded on \\([a,b]\\) and continuous on \\((a,b)\\). Let \\(F\\) be any function that is continuous on \\([a,b]\\) such that \\(F&#39;(x)=f(x)\\) on \\((a,b)\\). Then \\[\\int\\limits_a^bf(x)dx = F(b)-F(a)\\] \\(\\int\\limits_a^b f(x)dx\\): Find the indefinite integral \\(F(x)\\). Evaluate \\(F(b)-F(a)\\). Properties of Definite Integrals: 5.4 Integration by Substitution Sometimes the integrand doesn’t appear integrable using common rules and antiderivatives. A method one might try is integration by substitution, which is related to the Chain Rule. Suppose we want to find the indefinite integral \\(\\int g(x)dx\\) and assume we can identify a function \\(u(x)\\) such that \\(g(x)=f[u(x)]u&#39;(x)\\). Let’s refer to the antiderivative of \\(f\\) as \\(F\\). Then the chain rule tells us that \\(\\frac{d}{dx} F[u(x)]=f[u(x)]u&#39;(x)\\). So, \\(F[u(x)]\\) is the antiderivative of \\(g\\). We can then write \\[\\int g(x) dx= \\int f[u(x)]u&#39;(x)dx = \\int \\frac{d}{dx} F[u(x)]dx = F[u(x)]+c\\] Procedure to determine the indefinite integral \\(\\int g(x)dx\\) by the method of substitution: Identify some part of \\(g(x)\\) that might be simplified by substituting in a single variable \\(u\\) (which will then be a function of \\(x\\)). Determine if \\(g(x)dx\\) can be reformulated in terms of \\(u\\) and \\(du\\). Solve the indefinite integral. Substitute back in for \\(x\\) Substitution can also be used to calculate a definite integral. Using the same procedure as above, \\[\\int\\limits_a^b g(x)dx=\\int\\limits_c^d f(u)du = F(d)-F(c)\\] where \\(c=u(a)\\) and \\(d=u(b)\\). Example: \\(\\int x^2 \\sqrt{x+1}dx\\) The problem here is the \\(\\sqrt{x+1}\\) term. However, if the integrand had \\(\\sqrt{x}\\) times some polynomial, then we’d be in business. Let’s try \\(u=x+1\\). Then \\(x=u-1\\) and \\(dx=du\\). Substituting these into the above equation, we get \\[\\begin{eqnarray} \\int x^2\\sqrt{x+1}dx&amp;=&amp;\\int (u-1)^2\\sqrt{u}du\\nonumber\\\\ &amp;=&amp;\\int (u^2-2u+1)u^{1/2}du\\nonumber\\\\ &amp;=&amp;\\int (u^{5/2}-2u^{3/2}+u^{1/2})du\\nonumber \\end{eqnarray}\\] We can easily integrate this, since it’s just a polynomial. Doing so and substituting \\(u=x+1\\) back in, we get \\[\\int x^2\\sqrt{x+1}dx=2(x+1)^{3/2}\\left[\\frac{1}{7}(x+1)^2 - \\frac{2}{5}(x+1)+\\frac{1}{3}\\right]+c\\] For the above problem, we could have also used the substitution \\(u=\\sqrt{x+1}\\). Then \\(x=u^2-1\\) and \\(dx=2u du\\). Substituting these in, we get \\[\\int x^2\\sqrt{x+1}dx=\\int (u^2-1)^2 u 2u du\\] which when expanded is again a polynomial and gives the same result as above. Another Example \\(\\int\\limits_0^1 \\frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx\\) When an expression is raised to a power, it’s often helpful to use this expression as the basis for a substitution. So, let \\(u=1+e^{2x}\\). Then \\(du=2e^{2x}dx\\) and we can set \\(5e^{2x}dx=5du/2\\). Additionally, \\(u=2\\) when \\(x=0\\) and \\(u=1+e^2\\) when \\(x=1\\). Substituting all of this in, we get \\[\\begin{eqnarray} \\int\\limits_0^1 \\frac{5e^{2x}}{(1+e^{2x})^{1/3}}dx &amp;=&amp; \\frac{5}{2}\\int\\limits_2^{1+e^2}\\frac{du}{u^{1/3}}\\nonumber\\\\ &amp;=&amp; \\frac{5}{2}\\int\\limits_2^{1+e^2} u^{-1/3}du\\nonumber\\\\ &amp;=&amp; \\left. \\frac{15}{4} u^{2/3} \\right|_2^{1+e^2}\\nonumber\\\\ &amp;=&amp; 9.53\\nonumber \\end{eqnarray}\\] 5.5 Integration by Parts, or Ultraviolet Voodoo Another useful integration technique is {}, which is related to the Product Rule of differentiation. The product rule states that \\[\\frac{d}{dx}(uv)=u\\frac{dv}{dx}+v\\frac{du}{dx}\\] Integrating this and rearranging, we get \\[\\int u\\frac{dv}{dx}dx= u v - \\int v \\frac{du}{dx}dx\\] or \\[\\int u(x) v&#39;(x)dx=u(x)v(x) - \\int v(x)u&#39;(x)dx\\] More frequently remembered as \\[\\int u dv = u v - \\int v du\\] where \\(du=u&#39;(x)dx\\) and \\(dv=v&#39;(x)dx\\). For definite integrals: \\(\\int\\limits_a^b u\\frac{dv}{dx}dx = \\left. u v \\right|_a^b - \\int\\limits_a^b v \\frac{du}{dx}dx\\) Our goal here is to find expressions for \\(u\\) and \\(dv\\) that, when substituted into the above equation, yield an expression that’s more easily evaluated. Examples: "],
["optimization.html", "Chapter 6 Optimization 6.1 Quadratic Forms 6.2 Concavity of Quadratic Forms 6.3 Definiteness of Quadratic Forms 6.4 First Order Conditions 6.5 Second Order Conditions 6.6 Definiteness and Concavity 6.7 Global Maxima and Minima 6.8 Constrained Optimization 6.9 Equality Constraints 6.10 Inequality Constraints 6.11 Kuhn-Tucker Conditions", " Chapter 6 Optimization Topics: \\(\\bullet\\) Quadratic Forms \\(\\bullet\\) Definiteness of Quadratic Forms \\(\\bullet\\) Maxima and Minima in \\({\\bf R}^n\\) \\(\\bullet\\) First Order Conditions \\(\\bullet\\) Second Order Conditions \\(\\bullet\\) Global Maxima and Minima \\(\\bullet\\) Constrained Optimization \\(\\bullet\\) Equality Constraints \\(\\bullet\\) Inequality Constraints \\(\\bullet\\) Kuhn-Tucker Conditions Much of the material and examples for this lecture are taken from Simon &amp; Blume (1994) and Ecker &amp; Kupferschmid (1988) 6.1 Quadratic Forms Quadratic forms important because 1. Approximates local curvature around a point — e.g., used to identify max vs min vs saddle point. 2. Simple, so easy to deal with. 3. Have a matrix representation. Quadratic Form: A polynomial where each term is a monomial of degree 2 in any number of variables: \\[\\begin{align*} \\text{One variable: }&amp; Q(x_1) = a_{11}x_1^2\\\\ \\text{Two variables: }&amp; Q(x_1,x_2) = a_{11}x_1^2 + a_{12}x_1x_2 + a_{22}x_2^2\\\\ \\text{N variables: }&amp; Q(x_1,\\cdots,x_n)=\\sum\\limits_{i\\le j} a_{ij}x_i x_j \\end{align*}\\] which can be written in matrix terms: \\[\\begin{align*} \\text{One variable: }&amp; Q({\\bf x}) = x_1^T a_{11} x_1\\\\ \\text{N variables: }&amp; Q({\\bf x})=\\begin{pmatrix} x1&amp;x2&amp;\\cdots&amp;x_n\\end{pmatrix} \\begin{pmatrix} a_{11}&amp;\\frac{1}{2}a_{12}&amp;\\cdots&amp;\\frac{1}{2}a_{1n}\\\\ \\frac{1}{2}a_{12}&amp;a_{22}&amp;\\cdots&amp;\\frac{1}{2}a_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ \\frac{1}{2}a_{1n}&amp;\\frac{1}{2}a_{2n}&amp;\\cdots&amp;a_{nn} \\end{pmatrix} \\begin{pmatrix} x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{pmatrix}\\\\ &amp;={\\bf x}^T{\\bf A}{\\bf x} \\end{align*}\\] Examples: 6.2 Concavity of Quadratic Forms Concavity helps identify the curvature of a function, \\(f( x)\\), in 2 dimensional space. : The second derivative can be used to understand concavity. If \\[\\begin{array}{lll} f&#39;&#39;(x) &lt; 0 &amp; \\Rightarrow &amp; \\text{Concave}\\\\ f&#39;&#39;(x) &gt; 0 &amp; \\Rightarrow &amp; \\text{Convex} \\end{array}\\] 6.3 Definiteness of Quadratic Forms Definiteness helps identify the curvature of a function, \\(Q({\\bf x})\\), in n dimensional space. Definiteness: By definition, a quadratic form always takes on the value of zero when \\(x = 0\\), \\(Q({\\bf x})=0\\) at \\({\\bf x}=0\\). The definiteness of the matrix \\({\\bf A}\\) is determined by whether the quadratic form \\(Q({\\bf x})={\\bf x}^T{\\bf A}{\\bf x}\\) is greater than zero, less than zero, or sometimes both over all \\({\\bf x}\\ne 0\\). Examples: 6.4 First Order Conditions When we examined functions of one variable \\(x\\), we found critical points by taking the first derivative, setting it to zero, and solving for \\(x\\). For functions of \\(n\\) variables, the critical points are found in much the same way, except now we set the partial derivatives equal to zero. Gradient (\\(\\nabla \\fx\\)): Given a function \\(f({\\bf x})\\) in \\(n\\) variables, the gradient \\(\\nabla \\fx\\) is a column vector, where the \\(i\\)th element is the partial derivative of \\(f({\\bf x})\\) with respect to \\(x_i\\): \\[\\nabla \\fx = \\begin{pmatrix} \\frac{\\partial \\fx}{\\partial x_1}\\\\[9pt] \\frac{\\partial \\fx}{\\partial x_2}\\\\ \\vdots \\\\[3pt] \\frac{\\partial \\fx}{\\partial x_n} \\end{pmatrix}\\] Critical Point: \\({\\bf x}^*\\) is a critical point iff \\(\\nabla f({\\bf x}^*)=0\\). If the partial derivative of f(x) with respect to \\(x^*\\) is 0, then \\({\\bf x}^*\\) is a critical point. To solve for \\({\\bf x}^*\\), find the gradient, set each element equal to 0, and solve the system of equations. \\[{\\bf x}^* = \\begin{pmatrix} x_1^*\\\\x_2^*\\\\ \\vdots \\\\ x_n^*\\end{pmatrix}\\] Example: Given a function \\(\\fx=(x_1-1)^2+x_2^2+1\\), find the: \\end{itemize} 6.5 Second Order Conditions When we found a critical point for a function of one variable, we used the second derivative as an indicator of the curvature at the point in order to determine whether the point was a min, max, or saddle (second derivative test of concavity). For functions of \\(n\\) variables, we use as an indicator of curvature. Hessian (\\({\\bf H(x)}\\)): Given a function \\(f({\\bf x})\\) in \\(n\\) variables, the hessian \\({\\bf H(x)}\\) is an \\(n\\times n\\) matrix, where the \\((i,j)\\)th element is the second order partial derivative of \\(f({\\bf x})\\) with respect to \\(x_i\\) and \\(x_j\\): \\[{\\bf H(x)}=\\begin{pmatrix} \\frac{\\partial^2 \\fx}{\\partial x_1^2}&amp;\\frac{\\partial^2\\fx}{\\partial x_1 \\partial x_2}&amp; \\cdots &amp; \\frac{\\partial^2 \\fx}{\\partial x_1 \\partial x_n}\\\\[9pt] \\frac{\\partial^2 \\fx}{\\partial x_2 \\partial x_1}&amp;\\frac{\\partial^2\\fx}{\\partial x_2^2}&amp; \\cdots &amp; \\frac{\\partial^2 \\fx}{\\partial x_2 \\partial x_n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\[3pt] \\frac{\\partial^2 \\fx}{\\partial x_n \\partial x_1}&amp;\\frac{\\partial^2\\fx}{\\partial x_n \\partial x_2}&amp; \\cdots &amp; \\frac{\\partial^2 \\fx}{\\partial x_n^2}\\end{pmatrix}\\] Note that the hessian will be a symmetric matrix because \\(\\frac{\\partial \\fx}{\\partial x_1\\partial x_2} = \\frac{\\partial \\fx}{\\partial x_2\\partial x_1}\\). Also note that given that \\(\\fx\\) is of quadratic form, each element of the hessian will be a constant. Second Order Conditions:\\[6pt] Given a function \\(\\fx\\) and a point \\({\\bf x}^*\\) such that \\(\\nabla f({\\bf x}^*)=0\\), Example: We found that the only critical point of \\(\\fx=(x_1-1)^2+x_2^2+1\\) is at \\({\\bf x}^*=(1,0)\\). Is it a min, max, or saddle point? 6.6 Definiteness and Concavity Although definiteness helps us to understand the curvature of an n-dimensional function, it does not necessarily tell us whether the function is globally concave or convex. We need to know whether a function is globally concave or convex to determine whether a critical point is a global min or max. Testing for Global Concavity: We can use the definiteness of the Hessian to determine whether a function is globally concave or convex: Notice that the definiteness conditions must be satisfied over the entire domain. 6.7 Global Maxima and Minima Global Max/Min Conditions: Given a function \\(\\fx\\) and a point \\({\\bf x}^*\\) such that \\(\\nabla f({\\bf x}^*)=0\\), Note that showing that \\(\\bf H(x^*)\\) is negative semidefinite is not enough to guarantee \\({\\bf x}^*\\) is a local max. However, showing that \\(\\bf H(x)\\) is negative semidefinite for all \\({\\bf x}\\) guarantees that \\(x^*\\) is a global max. (The same goes for positive semidefinite and minima.)\\ Example: Take \\(f_1(x)=x^4\\) and \\(f_2(x)=-x^4\\). Both have \\(x=0\\) as a critical point. Unfortunately, \\(f&#39;&#39;_1(0)=0\\) and \\(f&#39;&#39;_2(0)=0\\), so we can’t tell whether \\(x=0\\) is a min or max for either. However, \\(f&#39;&#39;_1(x)=12x^2\\) and \\(f&#39;&#39;_2(x)=-12x^2\\). For all \\(x\\), \\(f&#39;&#39;_1(x)\\ge 0\\) and \\(f&#39;&#39;_2(x)\\le 0\\) — i.e., \\(f_1(x)\\) is globally convex and \\(f_2(x)\\) is globally concave. So \\(x=0\\) is a global min of \\(f_1(x)\\) and a global max of \\(f_2(x)\\). Example Given \\(f({\\bf x})=x_1^3-x_2^3+9x_1x_2\\), find any maxima or minima. 6.8 Constrained Optimization We have already looked at optimizing a function in one or more dimensions over the whole domain of the function. Often, however, we want to find the maximum or minimum of a function over some restricted part of its domain.\\ ex: Maximizing utility subject to a budget constraint Types of Constraints: For a function \\(f(x_1, \\dots, x_n)\\), there are two types of constraints that can be imposed: In any constrained optimization problem, the constrained maximum will always be less than or equal to the unconstrained maximum. If the constrained maximum is less than the unconstrained maximum, then the constraint is binding. Essentially, this means that you can treat your constraint as an equality constraint rather than an inequality constraint. For example, the budget constraint binds when you spend your entire budget. This generally happens because we believe that utility is strictly increasing in consumption, i.e. you always want more so you spend everything you have. Any number of constraints can be placed on an optimization problem. When working with multiple constraints, always make sure that the set of constraints are not pathological; it must be possible for all of the constraints to be satisfied simultaneously. \\[\\max_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2)\\] \\[\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2)\\] This tells us to maximize/minimize our function, \\(f(x_1,x_2)\\), with respect to the choice variables, \\(x_1,x_2\\), subject to the constraint. Example: \\[\\max_{x_1,x_2} f(x_1, x_2) = -(x_1^2 + 2x_2^2) \\text{ s.t. }x_1 + x_2 = 4\\] It is easy to see that the maximum occurs at \\((x_1, x_2) = (0,0)\\), but that does not satisfy the constraint. How should we proceed? 6.9 Equality Constraints Equality constraints are the easiest to deal with because we know that the maximum or minimum has to lie on the (intersection of the) constraint(s). The trick is to change the problem from a constrained optimization problem in \\(n\\) variables to an unconstrained optimization problem in \\(n + k\\) variables, adding variable for equality constraint. We do this using a lagrangian multiplier. Lagrangian function: The Lagrangian function allows us to combine the function we want to optimize and the constraint function into a single function. Once we have this single function, we can proceed as if this were an optimization problem.\\ For each constraint, we must include a (\\(\\lambda_i\\)) as an additional variable in the analysis. These terms are the link between the constraint and the Lagrangian function.\\ Given a set-up: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) = a\\] We define the Lagrangian function \\(L(x_1,x_2,\\lambda_1)\\) as follows: \\[L(x_1,x_2,\\lambda_1) = f(x_1,x_2) - \\lambda_1 (c(x_1,x_2) - a)\\] More generally, in : \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) - \\sum_{i=1}^k\\lambda_i(c_i(x_1,\\dots, x_n) - r_i)\\] \\ Note that above we subtract the lagrangian term we subtract the constraint constant from the constraint function. Occasionally, you may see the following alternative form of the Lagrangian, which is : \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) + \\sum_{i=1}^k\\lambda_i(r_i - c_i(x_1,\\dots, x_n))\\] Here we add the lagrangian term we subtract the constraing function from the constraint constant. : To find the critical points, we take the partial derivatives of lagrangian function, \\(L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k)\\), with respect to each of its variables (all choice variables \\({\\bf x}\\) all lagrangian multipliers \\({\\bf \\lambda}\\)). At a critical point, of these partial derivatives must be equal to zero, so we obtain a system of : \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial x_1} = \\frac{\\partial f}{\\partial x_1} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_1} &amp; = &amp; 0\\\\ \\vdots &amp; = &amp; \\vdots \\nonumber \\\\ \\frac{\\partial L}{\\partial x_n} = \\frac{\\partial f}{\\partial x_n} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_n} &amp; = &amp; 0\\\\ \\frac{\\partial L}{\\partial \\lambda_1} = c_1(x_i, \\dots, x_n) - r_1&amp; = &amp; 0\\\\ \\vdots &amp; = &amp; \\vdots \\nonumber \\\\ \\frac{\\partial L}{\\partial \\lambda_k} = c_k(x_i, \\dots, x_n) - r_k &amp; = &amp; 0 \\end{eqnarray*}\\] We can then solve this system of equations, because there are \\(n+k\\) equations and \\(n+k\\) unknowns, to calculate the critical point \\((x_1^*,\\dots,x_n^*,\\lambda_1^*,\\dots,\\lambda_k^*)\\). There may be more than one critical point, i.e. we need to verify that the critical point we find is a maximum/minimum. Similar to unconstrained optimization, we can do this by checking the second-order conditions. Example: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } x_1 + x_2 = 4\\] Notice that when we take the partial derivative of L with respect to the lagranigian multiplier and set it equal to 0, we return exactly our constraint! This is why signs matter. 6.10 Inequality Constraints Inequality constraints define the boundary of a region over which we seek to optimize the function. This makes inequality constraints more challenging because we do not know if the maximum/minimum lies along one of the constraints (the constraint binds) or in the interior of the region. We must introduce more variables in order to turn the problem into an unconstrained optimization. Slack: For inequality constraint \\(c_i(x_1, \\dots, x_n) \\leq a_i\\), we define a slack variable \\(s_i^2\\) for which the expression \\(c_i(x_1, \\dots, x_n) \\leq a_i - s_i^2\\) would hold with equality. These slack variables capture how close the constraint comes to binding. We use \\(s^2\\) rather than \\(s\\) to ensure that the slack is positive. Slack is just a way to transform our constraints. Given a set-up and these edited constraints: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) \\le a_1\\] Adding in Slack: \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } c(x_1,x_2) \\le a_1 - s_1^2\\] We define the Lagrangian function \\(L(x_1,x_2,\\lambda_1,s_1)\\) as follows: \\[L(x_1,x_2,\\lambda_1,s_1) = f(x_1,x_2) - \\lambda_1 ( c(x_1,x_2) + s_1^2 - a_1)\\] More generally, in : \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k, s_1, \\dots, s_k) = f(x_1, \\dots, x_n) - \\sum_{i = 1}^k \\lambda_i(c_i(x_1,\\dots, x_n) + s_i^2 - a_i)\\] : To find the critical points, we take the partial derivatives of the lagrangian function, \\(L(x_1,\\dots,x_n,\\lambda_1,\\dots,\\lambda_k,s_1,\\dots,s_k)\\), with respect to each of its variables (all choice variables \\(x\\), all lagrangian multipliers \\(\\lambda\\), and all slack variables \\(s\\)). At a critical point, of these partial derivatives must be equal to zero, so we obtain a system of : \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial x_1} = \\frac{\\partial f}{\\partial x_1} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_1} &amp; = &amp; 0\\\\ \\vdots &amp; = &amp; \\vdots \\nonumber \\\\ \\frac{\\partial L}{\\partial x_n} = \\frac{\\partial f}{\\partial x_n} - \\sum_{i = 1}^k\\lambda_i\\frac{\\partial c_i}{\\partial x_n} &amp; = &amp; 0\\\\ \\frac{\\partial L}{\\partial \\lambda_1} = c_1(x_i, \\dots, x_n) + s_1^2 - b_1&amp; = &amp; 0\\\\ \\vdots &amp; = &amp; \\vdots \\nonumber \\\\ \\frac{\\partial L}{\\partial \\lambda_k} = c_k(x_i, \\dots, x_n) + s_k^2 - b_k &amp; = &amp; 0\\\\ \\frac{\\partial L}{\\partial s_1} = 2s_1\\lambda_1 &amp; = &amp; 0\\\\ \\vdots &amp; = &amp; \\vdots \\nonumber \\\\ \\frac{\\partial L}{\\partial s_k} = 2s_k\\lambda_k &amp; = &amp; 0 \\end{eqnarray*}\\] : The last set of first order conditions of the form \\(2s_i\\lambda_i = 0\\) (the partials taken with respect to the slack variables) are known as complementary slackness conditions. These conditions can be satisfied one of three ways: Example: Find the critical points for the following constrained optimization: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } x_1 + x_2 \\le 4\\] Example: Find the critical points for the following constrained optimization: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } \\begin{array}{l} x_1 + x_2 \\le 4\\\\ x_1 \\ge 0\\\\ x_2 \\ge 0 \\end{array}\\] 6.11 Kuhn-Tucker Conditions As you can see, this can be a pain. When dealing explicitly with , this process is simplified by using the Kuhn-Tucker method. : Because the problem of maximizing a function subject to inequality and non-negativity constraints arises frequently in economics, the Kuhn-Tucker approach provides a method that often makes it easier to both calculate the critical points and identify points that are (local) maxima.\\ Given a : \\[\\max_{x_1,x_2}/\\min_{x_1,x_2} f(x_1,x_2) \\text{ s.t. } \\begin{array}{l} c(x_1,x_2) \\le a_1\\\\ x_1 \\ge 0 \\\\ gx_2 \\ge 0 \\end{array}\\] We define the Lagrangian function \\(L(x_1,x_2,\\lambda_1)\\) the same as if we did not have the non-negativity constraints: \\[L(x_1,x_2,\\lambda_2) = f(x_1,x_2) - \\lambda_1(c(x_1,x_2) - a_1)\\] More generally, in : \\[ L(x_1, \\dots, x_n, \\lambda_1, \\dots, \\lambda_k) = f(x_1, \\dots, x_n) - \\sum_{i=1}^k\\lambda_i(c_i(x_1,\\dots, x_n) - a_i)\\] : To find the critical points, we first calculate the by taking the partial derivatives of the lagrangian function, \\(L(x_1,\\dots,x_n,\\lambda_1,\\dots,\\lambda_k)\\), with respect to each of its variables (all choice variable s\\(x\\) and all lagrangian multipliers \\(\\lambda\\)) we calculate the by multiplying each partial derivative by its respective variable include for all variables (choice variables \\(x\\) and lagrangian multipliers \\(\\lambda\\)).\\ \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial x_1} \\leq 0, &amp; \\dots, &amp; \\frac{\\partial L}{\\partial x_n} \\leq 0\\\\ \\frac{\\partial L}{\\partial \\lambda_1} \\geq 0, &amp; \\dots, &amp; \\frac{\\partial L}{\\partial \\lambda_m} \\geq 0 \\end{eqnarray*}\\] \\[\\begin{eqnarray*} x_1\\frac{\\partial L}{\\partial x_1} = 0, &amp; \\dots, &amp; x_n\\frac{\\partial L}{\\partial x_n} = 0\\\\ \\lambda_1\\frac{\\partial L}{\\partial \\lambda_1} = 0, &amp; \\dots, &amp; \\lambda_m \\frac{\\partial L}{\\partial \\lambda_m} = 0 \\end{eqnarray*}\\] \\[\\begin{eqnarray*} x_1 \\geq 0 &amp; \\dots &amp; x_n \\geq 0\\\\ \\lambda_1 \\geq 0 &amp; \\dots &amp; \\lambda_m \\geq 0 \\end{eqnarray*}\\] Note that some of these conditions are set equal to 0, while others are set as inequalities!\\ Note also that to minimize the function \\(f(x_1, \\dots, x_n)\\), the simplest thing to do is maximize the function \\(-f(x_1, \\dots, x_n)\\); all of the conditions remain the same after reformulating as a maximization problem.\\ There are additional assumptions (notably, f(x) is quasi-concave and the constraints are convex) that are sufficient to ensure that a point satisfying the Kuhn-Tucker conditions is a global max; if these assumptions do not hold, you may have to check more than one point. : Given the above conditions, to find the critical points we solve the above system of equations. To do so, we must check border and interior solutions to see if they satisfy the above conditions. In a two-dimensional set-up, this means we must check the following cases: Example: \\[\\max_{x_1,x_2} f(x) = -(x_1^2 + 2x_2^2) \\text{ s.t. } \\begin{array}{l} x_1 + x_2 \\le 4\\\\ x_1 \\ge 0\\\\ x_2 \\ge 0 \\end{array}\\] Example: \\[\\max_{x_1,x_2} f(x) = \\frac{1}{3}\\log (x_1 + 1) + \\frac{2}{3}\\log (x_2 + 1) \\text{ s.t. } \\begin{array}{l} x_1 + 2x_2 \\leq 4\\\\ x_1 \\geq 0\\\\ x_2 \\geq 0 \\end{array}\\] "],
["references.html", "References", " References "],
["intro.html", "Chapter 7 Introduction", " Chapter 7 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 7. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 7.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 7.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 7.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 7.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "]
]
