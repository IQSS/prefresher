# Calculus I

__Topics__: 
 Sequences;
 Limit of a Sequence;
 Series;
 Derivatives;
 Higher-Order Derivatives;
 Composite Functions and The Chain Rule;
 Derivatives of Exp and Ln;
 Maxima and Minima;
 Partial Derivatives;
 L'H\^opital's Rule;
 Taylor Approximation;
 Derivative Calculus in 6 Steps
 

 Much of the material and examples for this lecture are taken from Simon \& Blume (1994) \emph{Mathematics for Economists} and from Boyce \& Diprima (1988) \emph{Calculus}



## Sequences


A \textbf{sequence} $\{y_n\}=\{y_1, y_2, y_3, \ldots, y_n\}$ is an ordered set of real numbers, where $y_1$ is the first term in the sequence and $y_n$ is the $n$th term. Generally, a sequence is \textbf{infinite}, that is it extends to $n=\infty$. We can also write the sequence as $\{y_n\}^\infty_{n=1}$.
	

Examples:
\begin{comment}
	\begin{enumerate}
		\item \parbox[t]{3in}{$\{y_n\}=\left\{ 2-\frac{1}{n^2} \right\} = $}\parbox[t]{1in}{\, {\includegraphics[width=1.5in]{limit.eps}}}
		\item \parbox[t]{3in}{$\{y_n\}=\left\{ \frac{n^2+1}{n} \right\} = $}\parbox[t]{1in}{\,  {\includegraphics[width=1.5in]{unbound.eps}}}
		\item \parbox[t]{3in}{$\{y_n\}=\left\{ (-1)^n \left(1-\frac{1}{n}\right) \right\} = $}\parbox[t]{1in}{\,  {\includegraphics[width=1.5in]{altrnate.eps}}}
	\end{enumerate}
\end{comment}


Think of sequences like functions.  Before, we had $y=f(x)$ with $x$ specified over some domain.  Now we have $\{y_n\}=\{f(n)\}$ with $n=1,2,3,\ldots$.

__Three kinds of sequences__:

1. Sequences like 1 above that converge to a limit.
2. Sequences like 2 above that increase without bound.
3. Sequences like 3 above that neither converge nor increase without bound --- alternating over the number line.

__Boundedness and monotonicity:__

1. \textbf{Bounded}: if $|y_n|\le K$ for all $n$
2. \textbf{Monotonically Increasing}: $y_{n+1}>y_n$ for all $n$
3. \textbf{Monotonically Decreasing}: $y_{n+1}<y_n$ for all $n$

\textbf{Subsequence}: Choose an infinite collection of entries from $\{ y_n \}$, retaining their order.





## The Limit of a Sequence
We're often interested in whether a sequence \textbf{converges} to a \textbf{limit}.  Limits of sequences are conceptually similar to the limits of functions addressed in the previous lecture.

\textbf{Limit of a sequence}.  The sequence $\{y_n\}$ has the limit $L$, that is $\lim\limits_{n \to \infty} y_n =L$, if for any $\epsilon>0$ there is an integer $N$ (which depends on $\epsilon$) with the property that $|y_n -L|<\epsilon$ for each $n>N$.  $\{y_n\}$ is said to converge to $L$.  If the above does not hold, then $\{y_n\}$ diverges.

\textbf{Uniqueness}: If $\{y_n\}$ converges, then the limit $L$ is unique.

\underline{Properties:} Let $\lim\limits_{n \to \infty} y_n = A$ and $\lim\limits_{n \to \infty} z_n =B$.  Then

\begin{enumerate}
		\item $\lim\limits_{n \to \infty} [\alpha y_n + \beta z_n]=\alpha A+\beta B$
		\item $\lim\limits_{n \to \infty} y_n z_n = A B$
		\item $\lim\limits_{n \to \infty} \frac{y_n}{z_n} = \frac{A}{B}$, provided $B\ne 0$
\end{enumerate}


Examples

\begin{enumerate}
		\item $\lim\limits_{n \to \infty} \left\{ 2-\frac{1}{n^2} 
		\right\} = $
		\item $\lim\limits_{n \to \infty} \left\{ \frac{4^n}{n!} \right\} 
		= $
\end{enumerate}

\begin{comment}
	\parbox{1in}{\,  {\includegraphics[width=1.5in]{limit2.eps}}}
\end{comment}
	


Finding the limit of a sequence in ${\bf R}^n$ is similar to that in ${\bf R}^1$.

\textbf{Limit of a sequence of vectors.} The sequence of vectors $\bf \{ y_n \}$ has the limit $\bf L$, that is $\lim\limits_{n\to\infty} {\bf y_n}={\bf L}$, if for any $\epsilon$ there is an integer $N$ where $\bf ||y_n-L||<\epsilon$ for each $n>N$.  The sequence of vectors $\bf \{ y_n\}$ is said to converge to the vector $\bf L$ --- and the distances between $\bf y_n$ and $\bf L$ converge to zero.


Think of each coordinate of the vector $\bf y_n$ as being part of its own sequence over $n$.  Then a sequence of vectors in ${\bf R}^n$ converges if and only if all $n$ sequences of its components converge.

Examples:

\begin{enumerate}
		\item The sequence $\{ y_n \}$ where $y_n=\left( \frac{1}{n}, 2-\frac{1}{n^2} \right)$ converges to $(0,2)$.
		\item The sequence $\{ y_n \}$ where $y_n=\left( \frac{1}{n}, (-1)^n \right)$ does not converge, since $\left\{ (-1)^n \right\}$ does not converge.
\end{enumerate}

\textbf{Bolzano-Weierstrass Theorem}: Any sequence contained in a compact (i.e., closed and bounded) subset of ${\bf R}^n$ contains a convergent subsequence.\\
		\parbox[t]{4in}{Example: Take the sequence $\{ y_n \} =\left\{ (-1)^n \right\}$, which has two accumulating points, but no limit.  However, it is both closed and bounded.}			
\begin{comment}
		\parbox{1.5in}{\,  {\includegraphics[width=1.5in]{bolzwei.eps}}}\\[12pt]
		\end{comment}
		\begin{comment}
		\parbox[t]{4in}{The subsequence of $\{ y_n \}$ defined by taking $n=1,3,5,\ldots$ does have a limit: $-1$.}\parbox{1.5in}{\,  {\includegraphics[width=1.5in]{bolzwei1.eps}}}\\[12pt]

\parbox[t]{4in}{As does the subsequence defined by taking $n=2,4,6,\ldots$, whose limit is $1$.}\parbox{1.5in}{\,  {\includegraphics[width=1.5in]{bolzwei2.eps}}}
\end{comment}



## Series

The sum of the terms of a sequence is a \textbf{series}. As there are both finite and infinite sequences, there are   \textbf{finite} and \textbf{infinite series}.

The series associated with the sequence $\{y_n\}=\{y_1, y_2,   y_3, \ldots, y_n\} = \{y_n\}_{n=1}^{\infty}$ is $\sum_{n=1}^{\infty} y_n$. The $n$th partial sum $S_n$ is defined as $S_n=\sum_{k=1}^n y_k$,the sum of the first $n$ terms of the sequence.

\textbf{Convergence:} A series $\sum y_n$ converges if the sequence of partial sums $\{S_1, S_2, S_3, ...\}$ converges, i.e has a finite limit. 

A \textbf{geometric series} is a series that can be written as $\sum_{n=0}^{\infty} r^n$, where $r$ is called the ratio. A geometric series converges to $\frac{1}{1-r}$ if $|r|< 1$ and diverges otherwise. For example, $\sum_{n=0}^{\infty} \frac{1}{2^n} = 2$.
	
Examples of other series:
\begin{enumerate}
		\item  $\sum_{n=0}^{\infty} \frac{1}{n!} = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots = e$.\\
		This one is especially useful in statistics and probability.
		\item $\sum_{n=1}^{\infty} \frac{1}{n} = \frac{1}{1} + \frac{1}{2}  + \frac{1}{3} + \cdots = \infty$ (harmonic series)
\end{enumerate}



## Derivatives

The derivative of $f$ at $x$ is its rate of change at $x$ --- i.e., how much $f(x)$ changes with a change in $x$.

* For a line, the derivative is the slope.\\
* For a curve, the derivative is the slope of the line tangent to the curve at $x$.


\textbf{Derivative}:  Let $f$ be a function whose domain includes an open interval containing the point $x$.  The derivative of $f$ at $x$ is given by

\begin{eqnarray}
		f'(x) &=&\lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{(x+h)-x}\nonumber\\
		&=&\lim\limits_{h\to 0} \frac{f(x+h)-f(x)}{h}\nonumber
\end{eqnarray}


If $f'(x)$ exists at a point $x$, then $f$ is said to be __differentiable__ at $x$.  Similarly, if $f'(x)$ exists for every point a long an interval, then $f$ is differentiable along that interval.  For $f$ to be differentiable at $x$, $f$ must be both continuous and "smooth'' at $x$.  The process of calculating $f'(x)$ is called __differentiation__.

__Notation for derivatives:__
\begin{enumerate}
		\item \parbox[t]{2in}{$y'$, $f'(x)$} (Prime or Lagrange Notation)
		\item \parbox[t]{2in}{$Dy$, $Df(x)$} (Operator Notation)
		\item \parbox[t]{2in}{$\frac{dy}{dx}$, $\frac{df}{dx}(x)$} (Leibniz's Notation)
\end{enumerate}


\underline{Properties of derivatives:}  Suppose that $f$ and $g$ are differentiable at $x$ and that $\alpha$ is a constant.  Then the functions $f\pm g$, $\alpha f$, $f g$, and $f/g$ (provided $g(x)\ne 0$) are also differentiable at $x$.  Additionally,

\begin{enumerate}
		\item \parbox[t]{2in}{\bf Power rule:} $[x^k]' = k x^{k-1}$  
		\item \parbox[t]{2in}{\bf Sum rule:} $[f(x)\pm g(x)]' = f'(x)\pm g'(x)$ 
		\item \parbox[t]{2in}{\bf Constant rule:} $[\alpha f(x)]' = \alpha f'(x)$ 
		\item \parbox[t]{2in}{\bf Product rule:} $[f(x)g(x)]' = f'(x)g(x)+f(x)g'(x)$
		\item \parbox[t]{2in}{\bf Quotient rule:} $[f(x)/g(x)]' = \frac{f'(x)g(x)-f(x)g'(x)}{[g(x)]^2}$, $g(x)\ne 0$\\
\end{enumerate}	
	
\begin{comment}
	\begin{framed}
	\item[] Examples:
	\begin{enumerate}
		\item $f(x)=c$\\
			 $f'(x)=$
		\item \parbox[c]{4in}{$f(x)=x$\\
			$f'(x)=$}
			\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{derivx.eps}}}
		\item \parbox[c]{4in}{$f(x)=x^2$\\
			$f'(x)=$}
			\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{derivx2.eps}}}
		\item \parbox[c]{4in}{$f(x)=x^3$\\
			$f'(x) = $}
			\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{derivx3.eps}}}
		\item $f(x)=3x^2+2x^{1/3}$\\
			$f'(x)=$\\
		\item $f(x)=(x^3)(2x^4)$\\
			$f'(x)=$\\\\ 
		\item $f(x)=\frac{x^2+1}{x^2-1}$\\
			$f'(x)=$
	\end{enumerate}
	\end{framed}
\end{comment}
	





## Higher-Order Derivatives or, Derivatives of Derivatives of Derivatives


We can keep applying the differentiation process to functions that are themselves derivatives.  The derivative of $f'(x)$ with respect to $x$, would then be $$f''(x)=\lim\limits_{h\to 0}\frac{f'(x+h)-f'(x)}{h}$$ and so on.  Similarly, the derivative of $f''(x)$ would be denoted $f'''(x)$.

\parbox[t]{2in}{{\bf First derivative}:} $f'(x)$, $y'$, $\frac{df(x)}{dx}$, $\frac{dy}{dx}$

\parbox[t]{2in}{{\bf Second derivative}:} $f''(x)$, $y''$, $\frac{d^2f(x)}{dx^2}$, $\frac{d^2y}{dx^2}$

\parbox[t]{2in}{{\bf $\bf n$th derivative}:}  $\frac{d^nf(x)}{dx^n}$, $\frac{d^ny}{dx^n}$


Example:
\begin{align*}
f(x) &=x^3
f'(x) &=3x^2
f''(x) &=6x 
f'''(x) &=6
f''''(x) &=0
\end{align*}


## Composite Functions and the Chain Rule
__Composite functions__ are formed by substituting one function into another and are denoted by $$(f\circ g)(x)=f[g(x)]$$  To form $f[g(x)]$, the range of $g$ must be contained (at least in part) within the domain of $f$. The domain of $f\circ g$ consists of all the points in the domain of $g$ for which $g(x)$ is in the domain of $f$.

Examples:

\begin{enumerate}
		\item \parbox[t]{2in}{$f(x)=\ln x$,}  \phantom{$0<x<\infty$}\\ 
			\parbox[t]{2in}{$g(x)=x^2$} \phantom{$-\infty<x<\infty$}\\ 
			\parbox[t]{2in}{$(f\circ g)(x)=\ln x^2$,} \phantom{$-\infty<x<\infty - \{0\}$}\\ 
			\parbox[t]{2in}{$(g\circ f)(x)=[\ln x]^2$,} \phantom{$0<x<\infty$}\\ 
				Notice that $f\circ g$ and $g\circ f$ are not the same functions.		
		\item \parbox[t]{2in}{$f(x)=4+\sin x$,} \phantom{$-\infty<x<\infty$}\\
			\parbox[t]{2in}{$g(x)=\sqrt{1-x^2}$,} \phantom{$-1\le x\le 1$}\\
			\parbox[t]{2in}{$(f\circ g)(x)=4+\sin \sqrt{1-x^2}$,} \phantom{$-1\le x\le 1$}\\ 
				$(g\circ f)(x)$ does not exist. $\sqrt{1-(4+\sin(x))^2}$ is not a real number because $1-(4+\sin(x))^2$ is always negative.
\end{enumerate}

\textbf{Chain Rule}:  Let $y=(f\circ g)(x)= f[g(x)]$. The derivative of $y$ with respect to $x$ is $$\frac{d}{dx} \{ f[g(x)] \} = f'[g(x)] g'(x)$$ which can also be written as $$\frac{dy}{dx}=\frac{dy}{dg(x)} \frac{dg(x)}{dx}$$ (Note: the above does not imply that the $dg(x)$'s cancel out, as in fractions.  They are part of the derivative notation and you can't separate them out or cancel them.)

The chain rule can be thought of as the derivative of the "outside" times the derivative of the "inside", remembering that the derivative of the outside function is evaluated at the value of the inside function.
	
	

\textbf{Generalized Power Rule}:  If $y=[g(x)]^k$, then $dy/dx=k[g(x)]^{k-1}g'(x)$.

\begin{framed}
Examples:
\begin{enumerate}
		\item Find $dy/dx$ for $y=(3x^2+5x-7)^6$. Let $f(z)=z^6$ and $z=g(x)=3x^2+5x-7$.  Then, $y=f[g(x)]$ and
		\begin{eqnarray*}
			\frac{dy}{dx}&=
		\end{eqnarray*}
		
		\item Find $dy/dx$ for $y=\sin(x^3+4x)$. (Note: the derivative of $\sin x$ is $\cos x$.)  Let $f(z)=\sin z$ and $z=g(x)=x^3+4x$.  Then, $y=f[g(x)]$ and
		\begin{eqnarray*}
			\frac{dy}{dx}&=
		\end{eqnarray*}
\end{enumerate}
\end{framed}




## Derivatives of Euler's number and natural logs

__Derivatives of Exp or $e$__:

\begin{enumerate}
		\item $\frac{d}{dx}e^x = e^x$
		\item $\frac{d}{dx}\alpha e^x = \alpha e^x$
		\item $\frac{d^n}{dx^n} \alpha e^x = \alpha e^x$
		\item $\frac{d}{dx}e^{u(x)}= e^{u(x)} u'(x)$
\end{enumerate}

Examples:  Find $dy/dx$ for
\begin{enumerate}
		\item \parbox[t]{1.5in}{$y=e^{-3x}$}\parbox[t]{4in}{\phantom{Let $u(x)=-3x$.  Then $u'(x)=-3$ and $dy/dx=-3e^{-3x}$.}} 
		\item \parbox[t]{1.5in}{$y=e^{x^2}$}\parbox[t]{4in}{\phantom{Let $u(x)=x^2$.  Then $u'(x)=2x$ and $dy/dx=2xe^{x^2}$.}} 
		\item \parbox[t]{1.5in}{$y=e^{\sin 2x}$}\parbox[t]{4in}{\phantom{Let $u(x)=\sin 2x$.  Then $u'(x)=2\cos 2x$ and $dy/dx=(2\cos 2x) e^{\sin 2x}$.}} 
\end{enumerate}

\textbf{Derivatives of $\ln$}:
\begin{enumerate}
		\item $\frac{d}{dx} \ln x = \frac{1}{x}$
		\item $\frac{d}{dx} \ln x^k = \frac{d}{dx} k \ln x = \frac{k}{x}$
		\item $\frac{d}{dx} \ln u(x) = \frac{u'(x)}{u(x)}\quad$  (by the chain rule)
		\item For any positive base $b$, $\frac{d}{dx} b^x = (\ln b)\left(b^x\right)$.
\end{enumerate}

Examples:  Find $dy/dx$ for
\begin{enumerate}
		\item \parbox[t]{1.5in}{$y=\ln(x^2+9)$}\parbox[t]{4in}{\phantom{Let $u(x)=x^2+9$.  Then $u'(x)=2x$ and $dy/dx=u'(x)/u(x)=2x/(x^2+9)$.}} 
		\item \parbox[t]{1.5in}{$y=\ln(\ln x)$}\parbox[t]{4in}{\phantom{Let $u(x)=\ln x$.  Then $u'(x)=1/x$ and $dy/dx=1/(x\ln x)$.}} 
		\item \parbox[t]{1.5in}{$y=(\ln x)^2$}\parbox[t]{4in}{\phantom{Use the generalized power rule. $dy/dx=(2\ln x)/x$.}} 
		\item \parbox[t]{1.5in}{$y=\ln e^x$}\parbox[t]{4in}{\phantom{(We know that $\ln e^x=x$ and that $dx/dx=1$, but let's double check.)  Let $u(x)=e^x$.  Then $u'(x)=e^x$ and $dy/dx=u'(x)/u(x)=e^x/e^x=1$.}} 
\end{enumerate}


## Applications of the Derivative: Maxima and Minima

The first derivative, $f'(x)$, identifies whether the function $f(x)$ at the point $x$ is increasing or decreasing at $x$.
\begin{enumerate}
		\item \parbox[t]{4in}{\bf Increasing:} $f'(x)>0$
		\item \parbox[t]{4in}{\bf Decreasing:} $f'(x)<0$
		\item \parbox[t]{4in}{{\bf Neither increasing nor decreasing}:} $f'(x)=0$\\
			 i.e. a maximum, minimum, or saddle point
\end{enumerate}

Examples:
	\begin{comment}
	\begin{enumerate}
		\item \parbox[c]{4.75in}{$f(x)=x^2+2$, $f'(x)=2x$}
			\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{deriv1.eps}}}
		\item \parbox[c]{4.75in}{$f(x)=x^3+2$, $f'(x)=3x^2$}
			\parbox[c]{1in}{\,  {\includegraphics[width=1in, angle = 270]{deriv2.eps}}}
	\end{enumerate}
	\end{comment}


The second derivative $f''(x)$ identifies whether the function $f(x)$ at the point $x$ is
\begin{enumerate}
		\item \parbox[t]{2in}{\bf Concave down:} $f''(x)<0$
		\item \parbox[t]{2in}{\bf Concave up:} $f''(x)>0$
\end{enumerate}

__Maximum (Minimum)__: $x_0$ is a __local maximum (minimum)__ if $f(x_0)>f(x)$ ($f(x_0)<f(x))$ for all $x$ within some open interval containing $x_0$.  $x_0$ is a __ global maximum (minimum)__ if $f(x_0)>f(x)$ ($f(x_0)<f(x))$ for all $x$ in the domain of $f$.

__Critical points__: Given the function $f$ defined over domain $D$, all of the following are defined as __critical points__:

1. Any interior point of $D$ where $f'(x)=0$.
2. Any interior point of $D$ where $f'(x)$ does not exist.
3. Any endpoint that is in $D$.

The maxima and minima will be a subset of the critical points.

__Second Derivative Test of Maxima/Minima__:  We can use the second derivative to tell us whether a point is a maximum or minimum of $f(x)$. 
\begin{itemize}
	\item[] \parbox[t]{2in}{\bf Local Maximum:} $f'(x)=0$ and $f''(x)<0$
	\item[] \parbox[t]{2in}{\bf Local Minimum:} $f'(x)=0$ and $f''(x)>0$
	\item[] \parbox[t]{2in}{\bf Need more info:} $f'(x)=0$ and $f''(x)=0$
\end{itemize}
		

\textbf{Global Maxima and Minima}. Sometimes no global max or min exists --- e.g., $f(x)$ not bounded above or below.  However, there are three situations where we can fairly easily identify global max or min.

\begin{enumerate}
\item {\bf Functions with only one critical point.} If $x_0$ is a local max or min of $f$ and it is the only critical point, then it is the global max or min.
\item {\bf Globally concave up or concave down functions.}  If $f''(x)$ is never zero, then there is at most one critical point. That critical point is a global maximum if $f''<0$ and a global minimum if $f''>0$.
\item {\bf Functions over closed and bounded intervals} must have both a global maximum and a global minimum.
\end{enumerate}


Examples: Find any critical points and identify whether they're a max, min, or saddle point:
\begin{enumerate}
		\item \parbox[t]{1.25in}{$f(x)=x^2+2$\\
			$$\\
			$$\\[7pt]}
			\parbox[t]{4.5in}{}
		
		\item \parbox[t]{1.25in}{$f(x)=x^3+2$\\
			$$\\
			$$\\[50pt]} 
			\parbox[t]{4.5in}{}
		
		\item \parbox[c]{3.5in}{$f(x)=|x^2-1|$, $x\in [-2,2]$\\
		}
		\begin{comment}
			\parbox{1in}{\,  {\includegraphics[width=1in, angle = 270]{absx2m1.eps}}}
			\end{comment}
			
			\item[] \parbox[c]{5.5in}{}
\end{enumerate}




## Partial Derivatives

Suppose we have a function $f$ now of two (or more) variables and we want to determine the rate of change relative to one of the variables. To do so, we would find it's partial derivative, which is defined similar to the derivative of a function of one variable. 

__Partial Derivative__:  Let $f$ be a function of the variables $(x_1,\ldots,x_n)$.  The partial derivative of $f$ with respect to $x_i$ is 

\[\frac{\partial f}{\partial x_i} (x_1,\ldots,x_n) = \lim\limits_{h\to 0} \frac{f(x_1,\ldots,x_i+h,\ldots,x_n)-f(x_1,\ldots,x_i,\ldots,x_n)}{h}\]
Only the $i$th variable changes --- the others are treated as constants.

We can take higher-order partial derivatives, like we did with functions of a single variable, except now we the higher-order partials can be with respect to multiple variables.

\begin{framed}
Examples:
\begin{enumerate}
		\item $f(x,y)=x^2+y^2$\\
			$\frac{\partial f}{\partial x}(x,y)=$\\
			$\frac{\partial f}{\partial y}(x,y)=$\\
			$\frac{\partial^2 f}{\partial x^2}(x,y)=$\\
			$\frac{\partial^2 f}{\partial x \partial y}(x,y)=$
		\item $f(x,y)=x^3 y^4 +e^x -\ln y$\\
			$\frac{\partial f}{\partial x}(x,y)=$\\
			$\frac{\partial f}{\partial y}(x,y)=$\\
			$\frac{\partial^2 f}{\partial x^2}(x,y)=$\\
			$\frac{\partial^2 f}{\partial x \partial y}(x,y)=$
	\end{enumerate}
\end{framed}



## L'H\^opital's Rule

In studying limits, we saw that $\lim\limits_{x \to c} f(x)/g(x) = \left(\lim\limits_{x \to c} f(x)\right)/\left(\lim\limits_{x \to c} g(x)\right)$, provided that $\lim\limits_{x \to c} g(x)\ne 0$.
	
If both $\lim\limits_{x \to c} f(x)=0$ and $\lim\limits_{x \to c} g(x)=0$, then we get an __indeterminate form__ of the type $0/0$ as $x\to c$.  However, a limit may still exist. We can use L'H\^opital's rule to find the limit.
	
__L'H\^opital's Rule__:  Suppose $f$ and $g$ are differentiable on some interval $a<x<b$ and that either

\begin{enumerate}
		\item $\lim\limits_{x\to a^+} f(x)=0$ and $\lim\limits_{x\to a^+} g(x)=0$, or
		\item $\lim\limits_{x\to a^+} f(x)=\pm\infty$ and $\lim\limits_{x\to a^+} g(x)=\pm\infty$\
\end{enumerate}

Suppose further that $g'(x)$ is never zero on $a<x<b$ and that 
\[\lim\limits_{x\to a^+} \frac{f'(x)}{g'(x)}=L\] then
\[\lim\limits_{x\to a^+} \frac{f(x)}{g(x)}=L\]
	
%Put more simply, if $\lim\limits_{x\to a f(x)}$ is of the form $0/0$ or $\pm \infty / \pm \infty$, then $$\lim\limits_{x\to a} f(x) = \lim\limits_{x\to a} f'(x)$$\\
	
And if $\lim\limits_{x\to a} \frac{f'(x)}{g'(x)} = 0/0$ or $\pm \infty / \pm \infty$ then you can apply L'H\^opital's rule a second time, and continue applying it until you have a solution.

\begin{framed}
Examples:  Use L'H\^opital's rule to find the following limits:
	\begin{enumerate}
		\item \parbox[t]{1.5in}{$\lim\limits_{x\to 0^+}\frac{\ln(1+x^2)}{x^3}$ \\[15pt]}
			\parbox[t]{4in}{}

		\item \parbox[t]{1.5in}{$\lim\limits_{x\to  0^+} \frac{e^{1/x}}{1/x}$ \\[15pt]}
			\parbox[t]{4in}{}
		
		\item \parbox[t]{1.5in}{$\lim\limits_{x\to 2} \frac{x-2}{(x+6)^{1/3}-2}$ \\[15pt]}
			\parbox[t]{4in}{}
	\end{enumerate}
\end{framed} 


## Taylor Series Approximation

\textbf{Taylor series} (also known as the delta method) are used commonly to represent functions as infinite series of the function's derivatives at some point $a$. For example, Taylor series are very helpful in representing _nonlinear_ functions as linear functions. One can thus _approximate_ functions by using lower-order, finite series known as \textbf{Taylor polynomials}. If $a=0$, the series is called a Maclaurin series.
  

Specifically, a Taylor series of a real or complex function  $f(x)$ that is infinitely differentiable in the neighborhood of point $a$ is: 


\begin{align*}
	f(x) &= f(a) + \frac{f'(a)}{1!} (x-a) +  \frac{f''(a)}{2!} (x-a)^2 + \cdots\\
	 &= \sum_{n=0}^\infty \frac{f^{(n)} (a)}{n!} (x-a)^n
\end{align*}
  
__Taylor Approximation__: We can often approximate the curvature of a function $f(x)$ at point $a$ using a 2nd order Taylor polynomial around point $a$: 

\[f(x) = f(a) + \frac{f'(a)}{1!} (x-a) +  \frac{f''(a)}{2!} (x-a)^2
+ R_2\]


$R_2$ is the Lagrange remainder and often treated as negligible,
giving us:

\[f(x) \approx f(a) + f'(a)(x-a) +  \dfrac{f''(a)}{2} (x-a)^2\]

Taylor series expansion is easily generalized to multiple dimensions.\\

\textbf{Curvature and The Taylor Polynomial as a Quadratic Form}: The Hessian is used in a Taylor polynomial approximation to $\fx$ and provides information about the curvature of $\fx$ at $\bm{x}$ --- e.g., which tells us whether a critical point $\bm{x}^*$ is a min, max, or saddle point.

\begin{enumerate}
  \item The second order Taylor polynomial about the critical point
${\bf x}^*$ is
  $$f({\bf x}^*+\bf h)=f({\bf x}^*)+\nabla f({\bf x}^*) \bf h +\frac{1}{2} \bf h^T
{\bf H(x^*)} \bf h + R(\bf h)$$
  \item Since we're looking at a critical point, $\nabla f({\bf x}^*)=0$;
and for small $\bf h$, $R(\bf h)$ is negligible.  Rearranging, we get
$$f({\bf x}^*+\bf h)-f({\bf x}^*)\approx \frac{1}{2} \bf h^T {\bf H(x^*)}
\bf h $$
  \item The RHS is a quadratic form and we can determine the definiteness of $\bf
H(x^*)$.
\end{enumerate}

## Summary: Derivative calculus in 6 steps

With these six rules (and decent algebra and trigonometry skills) you can figure out the derivative of anything.

1. Sum rule: \[[f(x)\pm g(x)]' = f'(x)\pm g'(x)\]
2. Product rule: \[[f(x)g(x)]' = f'(x)g(x)+f(x)g'(x)\]
3. Power rule: \[[x^k]' = k x^{k-1}\]
4. Chain rule: \[\frac{d}{dx} \{ f[g(x)] \} = f'[g(x)] g'(x)\]
5. $e^x$: \[\frac{d}{dx} e^x = e^x\]
6. Trig identity: \[\frac{d}{dx} \sin(x) = \cos(x)\]
