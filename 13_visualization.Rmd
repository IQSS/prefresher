# Visualization {#dataviz}

Module originally written by Shiro Kuriwaki


## Where are we? Where are we headed?

Up till now, you should have covered:

* The R Visualization and Programming primers at  <https://rstudio.cloud/primers/>
* Reading and handling data
* Matrices and Vectors


Today we'll cover:

* Visualization. 
* Data Wrangling

## Check your understanding 

* What does `:` mean in R? What about `==`? `,`?, `!=` , `&`, `|`, `%in% `
* What does `%>%` do
* How do you make a barplot, in base-R and in ggplot?


```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(scales)
```



## Motivation

In this module, let's visualize some cross-sectional stats with an actual Census. Then, we'll do an example on time trends with Supreme Court ideal points. 


Why care about the Census? The Census is one of the fundamental acts of a government. See the Law Review article by [Persily (2011)](http://cardozolawreview.com/Joomla1.5/content/32-3/Persily.32-3.pdf), "The Law of the Census"^[[Persily, Nathaniel. 2011. "The Law of the Census: How to Count, What to Count, Whom to Count, and Where to Count Them.”](http://cardozolawreview.com/Joomla1.5/content/32-3/Persily.32-3.pdf). _Cardozo Law Review_ 32(3): 755–91.]. The Census is government's primary tool for apportionment (allocating seats to districts), appropriations (allocating federal funding), and tracking demographic change.  See^[[Hochschild, Jennifer L., and Brenna Marea Powell. 2008. "Racial Reorganization and the United States Census 1850–1930: Mulattoes, Half-Breeds, Mixed Parentage, Hindoos, and the Mexican Race."](https://dash.harvard.edu/bitstream/handle/1/3153295/hoschschild_racialreorganization.pdf?sequence=2). _Studies in American Political Development_ 22(1): 59–96.] for example [Hochschild and Powell (2008)](https://dash.harvard.edu/bitstream/handle/1/3153295/hoschschild_racialreorganization.pdf?sequence=2) on how the categorizations of race in the Census during 1850-1930.


Time series data is a common form of data in social science data, and there is growing methodological work on making causal inferences with time series^[[Blackwell, Matthew,  and Adam Glynn. 2018. "How to Make Causal Inferences with Time-Series Cross-Sectional Data under Selection on Observables."](https://doi.org/10.1017/S0003055418000357)  _American Political Science Review_]. 



## Read data

First, the census.


The first of line code you will write is often to read in data. Here, let's read in a subset of the 2010 Census. This is a 0.01 percent random sample of the entire U.S. Census. 


```{r}
cen10 <- read_csv("input/usc2010_001percent.csv", col_types = cols())
```

The data comes from IPUMS^[[Ruggles, Steven, Katie Genadek, Ronald Goeken, Josiah Grover, and Matthew Sobek. 2015. Integrated Public Use Microdata Series: Version 6.0 dataset](http://doi.org/10.18128/D010.V6.0)], a great source to extract and analyze Census and Census-conducted survey (ACS, CPS) data.


What does this data look like?
```{r}
cen10
```



## Counting
How many people are in your sample. Three ways to count:
```{r}
nrow(cen10)
```


```{r}
dim(cen10)
```


```{r}
length(cen10$year)
```


Multiply this number by the inverse of the sample proportion. Does it equal the total population of the U.S. in 2010^[The 2010 Census reported 308,745,539. [Wikipedia](https://en.wikipedia.org/wiki/2010_United_States_Census)]? 
```{r}
nrow(cen10)*100*100
```

This and all subsequent  tasks involve manipulating and summarizing data, sometimes called "wrangling". There are at least two approaches, "base-R" and "tidyverse functions". The tidyverse functions are add-ons that programmers have built, and appears to be popular.

## Tabulating
Here are two ways to count by group, or to tabulate.

Use the `table` function
```{r}
table(cen10$race)
```


With tidyverse, use `group_by` to group by a variable, and then `summarize` to count. 
```{r}
cen10 %>% 
  group_by(race) %>%
  summarize(count = n())
```

These `tidyverse` commands from the `dplyr` package are newer and not built-in, but they are one of the increasingly more popular ways to wrangle data. 

* 80 percent of your data wrangling needs might be doable with these basic `dplyr` functions: `select`, `mutate`, `group_by`, `summarize`, and `arrange`.
* These verbs roughly correspond to the same commands in SQL, another important language in data science. 
* The `%>%` symbol is a pipe. It takes the thing on the left side and pipes it down to the function on the right side. `cen10 %>% group_by(race)` means take `cen10` and pass it on to the function `group_by`, which will group observations by race. Passing that (`%>%`) to `summarize(count = n())` means to then summarize the grouped output by the variable `count`, which we define by the number of rows `n()`. 


## base R and ggplot
Visualizing data is the key part of communication; good data viz gets the point across^[[Kastellec, Jonathan P., and Eduardo L. Leoni. 2007. "Using Graphs Instead of Tables in Political Science."](http://www.princeton.edu/~jkastell/Tables2Graphs/graphs.pdf).  _Perspectives on Politics_ 5 (4): 755–71.]. 

Two prevalent ways of making graphing are referred to as "base-R" and "ggplot".

### base R

"Base-R"  graphics are graphics that are made with R's default graphics commands. First, let's assign our tabulation to an object, call it `tab_race`

```{r}
tab_race <- table(cen10$race)
``` 

Then put it in the `barplot()` function
```{r, fig.fullwidth = TRUE}
barplot(tab_race)
```


### ggplot
A popular alternative a `ggplot` graphics. `gg` stands for grammar of graphics by Hadley Wickham, and it has a new semantics of explaining graphics in R. Again, first let's set up the data. 

Let's group and count first, like we just did.

```{r, fig.fullwidth = TRUE}
grp_race <- group_by(cen10, race) %>%
  summarize(count = n())
```

We will now plot this grouped set of numbers. The `ggplot()` function takes two main arguments, `data` and `aes`. 

1. First enter a single dataframe from which you will draw a plot.
2. Then enter the `aes`, or aesthetics. This defines which variable in the data the plotting functions should take for pre-set dimensions in graphics. The dimensions `x` and `y` are the most important. We will assign `race` and `count` to them, respectively,
3. After you close `ggplot()` .. add __layers__ by the plus sign. A `geom` is a layer of graphical representation, for example `geom_histogram` renders a histogram, `geom_point` renders a scatter plot. For a barplot, we can use `geom_col()`


```{r, fig.fullwidth = TRUE}
ggplot(data = grp_race, aes(x = race, y = count)) + geom_col()
```


## Improving your graphics
Adjusting your graphics to make the point clear is an important skill. Here is an example of showing the same numbers but with a different design, in a way that aims to maximize the "data-to-ink ratio".


```{r, fig.fullwidth = TRUE}
par(oma = c(1, 10, 1, 1))
barplot(sort(tab_race), # sort numbers
        horiz = TRUE, # flip
        border = NA, # border is extraneous
        xlab = "Number in Race Category", 
        bty = "n", # no box
        las = 1) # alignment of axis labels is horizontal
```

Notice that we applied the `sort()` function to order the bars in terms of their counts. The default ordering of a categorical variable / factor is alphabetical. Alphabetical ordering is uninformative and almost never the way you should order variables.

In ggplot you might do this by:
```{r}
grp_race_ordered <- arrange(grp_race, count) %>% 
  mutate(race = forcats::as_factor(race))

ggplot(data = grp_race_ordered, aes(x = race, y = count)) +
  geom_col() +
  coord_flip() +
  labs(y = "Number in Race Category",
       x = "",
       caption = "Source: 2010 U.S. Census sample")

```


The data ink ratio was popularized by Ed Tufte (originally a political economy scholar who has recently become well known for his data visualization work). See Tufte (2001),  _The Visual Display of Quantitative Information_ and his website <https://www.edwardtufte.com/tufte/>.  For a R and ggplot focused example using social science examples, check out Healy  (2018). _Data Visualization: A Practical Introduction_ with a draft at <https://socviz.co/>  There are a growing number of excellent books on data visulization.


## Cross-tabs

Visualizations and Tables each have their states. A rule of thumb is that more than a dozen numbers on a table is too much to digest, but less than a dozen is too few for a figure to be worth it. Let's look at a table first. 


A cross-tab is counting with two types of variables, and is a simple and powerful tool to show the relationship between multiple variables.

```{r}
xtab_race_state <- table(cen10$race, cen10$state)
dim(xtab_race_state)
```

What if we care about proportions within states, rather than counts. We want to compare the racial composition of a small state (like Delaware) and a large state (like California). 

One way to transform a table of counts to a table of proportions is the function `prop.table`. Be careful what you want to take proportions of -- this is set by the `margin` argument. In R, the first margin (1) is _rows_ and the second margin (2) is columns.

```{r}
ptab_race_state <- prop.table(xtab_race_state, margin = 2)
dim(ptab_race_state)
```


## Composition Plots

To make the same figure with `ggplot()`. 

First, we want a count for each state $\times$ race combination.  So group by those two factors and count how many observations are in each two-way categorization.
```{r}
grp_race_state <- cen10 %>% 
  group_by(race, state) %>% 
  summarize(count = n())
```

Can you tell from the code what `grp_race_state` will look like?

```{r, eval = FALSE}
# run on your own
grp_race_state
```


Now, we want to tell `ggplot2` something like the following: I want bars by state, where heights indicate racial groups. Each bar should be colored by the race. With some googling, you will get something like this 
```{r, fig.height = 8}
ggplot(data = grp_race_state, aes(x = state, y = count,  fill = race)) +
  geom_col(position = "fill") + # the position is dertemined by the fill ae
  scale_fill_brewer(palette = "OrRd", direction = -1) + # choose palette
  coord_flip() + # flip axes
  scale_y_continuous(labels = percent) + # label numbers as percentage
  labs(y = "Proportion of Racial Group within State",
       x = "",
       source = "Source: 2010 Census  sample")
```


## Histograms
When our data is continuous rather than categorical, our first graphics will probably be a histogram.

```{r}
hist(cen10$age, main = "Age Distribution", xlab = "Age")
```


```{r}
ggplot(data = cen10, aes(x = age)) +
  geom_histogram()
```



## Line graphs

Line graphs are useful for plotting time trends.  

The Census does not track individuals over time. So let's take up another example: The U.S. Supreme Court. Take the dataset `justices_median.csv`.

This data is adapted from the estimates of Martin and Quinn on their website <http://mqscores.lsa.umich.edu/>  


```{r, message=FALSE}
justice <- read_csv("input/justices_court-median.csv")
```

What does the data look like? How do you think it is organized? What does each row represent?

```{r}
justice
```



As you might have guessed, these data can be shown in a time trend from the range of the `term` variable. As there are only nine justices at any given time and justices have life tenure, there times on the court are staggered. With a common measure of "preference", we can plot time trends of these justices ideal points on the same y-axis scale. 


```{r}
ggplot(justice, aes(x = term, y = idealpt, group = justice)) +
  geom_line()
```

This seems to "work" off the shelf. But take a moment to see why the code was written as it is and how that maps on to the graphics. In particular, what is the `group` aesthetic doing for you? What would happen if that was not specified?

Now, this graphic already indicates a lot, but let's improve the graphics so people can actually read it.  This is left for Exercise 3. 


As social scientists, we should also not forget to ask ourselves whether these numerical measures are fit for what we care about, or actually succeeds in measuring what we'd like to measure. The estimation of these "ideal points" is a subfield of political methodology beyond this prefresher. For more reading, skim through the original paper by Martin and Quinn (2002)^[[Martin, Andrew D. and Kevin M. Quinn. 2002. "Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999"](http://mqscores.lsa.umich.edu/media/pa02.pdf). _Political Analysis._ 10(2): 134-153.]. Also for a methodological discussion on the difficulty of measuring time series of preferences, check out Bailey (2013)^[[Bailey, Michael A. 2013. "Is Today’s Court the Most Conservative in Sixty Years? Challenges and Opportunities in Measuring Judicial Preferences." ](https://michaelbailey.georgetown.domains/wp-content/uploads/2018/05/JOP_proofs_June2013.pdf). _Journal of Politics_ 75(3): 821-834]




## Exercises
In the time remaining, try the following exercises. Order doesn't matter. 

### Exercise 1: Make your own graphic
Make a graphic from the census data that tells us something interesting about the U.S. 2010 population.
```{r}
# Enter yourself

```


### Exercise 2: The Swing Justice

Using the `justice` dataset and building off of the plot that was given, make an improved plot that has the following features. 

* Plots each line in translucent gray, so the overlapping lines can be visualized clearly.
* Use a black-white background.
* Label axes
* Change the breaks of the x-axis to print numbers for every decade, not just every two decades.
* Limit the scale of the y-axis to [-5, 5] so that the outlier justice in the 60s is trimmed and the rest of the data can be seen more easily (also, who is that justice?)
* Plot the ideal point of the justice who holds the "median" ideal point in a given term. To distinguish this with the others, plot this line separately in a very light red _below_ the individual justice's lines.
* Highlight the trendline of only the nine justices who are _currently_ sitting on SCOTUS. Make sure this is clearer than the other past justices. 
* Add the current nine justice's names to the right of the endpoint of the 2016 figure, alongside their ideal point. 
* Make sure the text labels do not overlap with each other for readability using the `ggrepel` package. 
* Extend the x-axis label to about 2020 so the text labels of justices are to the right of the trendlines.
* Add a caption to your text describing the data briefly, as well as any features relevant for the reader (such as the median line and the trimming of the y-axis)


```{r}
# Enter yourself

``` 


### Exercise 3: Back to Census and Counting CVAP

A issue raised in Persily's article is that the full-count U.S. Census does not record  whether the residents are citizens of the United States^[Here is that argument of his again, more recently in the popular press. ["The Mysterious Number of American Citizens". June 2, 2015.  _POLITICO_](http://www.politico.com/magazine/story/2015/06/the-supreme-courts-big-data-problem-118568)]. Instead, this question is asked in a survey, the American Community Survey. The two are fundamentally different exercises: the Census counts everyone by definition, a survey samples its data. 

Load the 1 percent sample of the 2015 ACS (`acs2015_1percent.csv`, in the `input` folder) and give an estimate of the proportion of a state's ACS respondents that are reportedly U.S. citizens. 

```{r}
# Enter yourself

```
